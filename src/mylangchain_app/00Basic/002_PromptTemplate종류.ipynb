{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate \n",
    "* [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate)\n",
    "* [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)\n",
    "* [ChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html#langchain_core.prompts.chat.ChatMessagePromptTemplate)\n",
    "* [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html#langchain_core.prompts.few_shot.FewShotPromptTemplate)\n",
    "* PartialPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_6\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) PromptTemplate 의 from_template() 함수 사용\n",
    "* 주로 LLM(텍스트 완성형 모델, ex. Ollama, GPT-3.5)과 함께 사용\n",
    "* 하나의 문자열 프롬프트를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ChatGPT는 방대한 텍스트 데이터를 이용해 다음에 올 단어를 예측하도록 학습하는 사전 훈련(pre‑training) 과정을 거칩니다. '\n",
      " '이 과정에서 Transformer 구조와 자기‑주의(self‑attention) 메커니즘을 활용해 문맥을 이해하고 정보를 효율적으로 '\n",
      " '통합합니다. 이후 인간 피드백을 기반으로 한 강화 학습(RLHF) 등을 통해 모델을 미세 조정하여 보다 안전하고 유용한 응답을 생성하도록 '\n",
      " '최적화합니다.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pprint import pprint\n",
    "\n",
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    # model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) PromptTemplate 결합하기\n",
    "* 동일한 Prompt 패턴을 사용하지만 여러 개의 질문을 작성해서 LLM을 실행할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['count', 'language', 'model_name'] input_types={} partial_variables={} template='{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.'\n",
      "('ChatGPT는 인터넷의 방대한 글을 미리 학습해 두고, 사용자가 입력한 문장 다음에 올 법한 단어들의 확률을 계산해 문장을 만들어 '\n",
      " '냅니다.  \\n'\n",
      " '학습 때는 “마스크”로 일부 단어를 가려 놓고 이를 맞추는 방식으로 언어 패턴을 익히며, 대화 맥락을 유지하기 위해 트랜스포머라는 구조를 '\n",
      " '반복 활용합니다.  \\n'\n",
      " '이후 사람이 직접 대화 품질을 평가해 주는 “강화학습”을 거치면서 더 자연스럽고 유용한 답변을 만들어내도록 미세조정됩니다.\\n'\n",
      " '\\n'\n",
      " 'ChatGPT 모델의 장점 요약\\n'\n",
      " '• 문맥을 길게 기억해 자연스럽고 연결된 대화 가능  \\n'\n",
      " '• 다양한 주제에 대해 높은 이해도와 논리적 답변 제공  \\n'\n",
      " '• 코드 작성, 번역, 요약, 아이디어 생성 등 활용도가 뛰어남  \\n'\n",
      " '• 사용자 의도에 따라 톤·스타일을 유연하게 조절 가능  \\n'\n",
      " '• 지속적인 강화학습·업데이트로 성능이 빠르게 개선됨\\n'\n",
      " '\\n'\n",
      " 'ChatGPT와 비슷한 AI 모델\\n'\n",
      " '• 구글 제미나이(Gemini)  \\n'\n",
      " '• 메타 라마(Llama)  \\n'\n",
      " '• 클라우드 앤서로픽 클라우드(Claude)  \\n'\n",
      " '• 카카오 브레인 코넥트(KoGPT)')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"한국어\")\n",
    "print(combined_prompt)\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"한국어\"})\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate 의 파라미터를 배열 형태로 하여 여러개 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 3 문장으로 한국어로 답변해 주세요.', 'Gemini 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.', 'claude 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.']\n",
      "<class 'str'> GPT-4 모델의 학습 원리를 3 문장으로 한국어로 답변해 주세요.\n",
      "('GPT-4는 방대한 텍스트 데이터를 바탕으로 단어·문맥 간 확률 관계를 학습하는 ‘Transformer’ 아키텍처를 이용합니다.  \\n'\n",
      " '학습 시 입력 문장의 일부를 가리고 다음 토큰을 맞히는 ‘자기 지도 학습’ 방식으로 모델 내부 가중치를 반복 조정합니다.  \\n'\n",
      " '이렇게 얻은 패턴과 지식을 바탕으로 새로운 입력에 대해 가장 가능성 높은 다음 단어를 순차적으로 생성합니다.')\n",
      "<class 'str'> Gemini 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.\n",
      "('Gemini는 대규모 텍스트·이미지·오디오 등 다중 모달 데이터를 동시에 학습해 패턴을 포착합니다.  \\n'\n",
      " 'Transformer 기반 아키텍처로, 입력 토큰 간 관계를 어텐션 메커니즘으로 계산하며 다음 토큰을 예측합니다.  \\n'\n",
      " '강화학습과 인간 피드백(RLHF)으로 정답 경향을 반영해 보상을 최대화하며 정밀도를 높입니다.  \\n'\n",
      " '전체 파라미터를 대상으로 분산 처리하며, 학습률 스케줄링과 정규화로 과적합을 억제해 확장 가능합니다.')\n",
      "<class 'str'> claude 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.\n",
      "('클로드는 대규모 언어 모델로, 인터넷·책 등 방대한 텍스트를 통해 다음 토큰이 무엇일지를 확률적으로 예측하는 방식으로 학습합니다.  \\n'\n",
      " '학습 과정에서 트랜스포머 아키텍처의 어텐션 메커니즘이 문맥을 기억·활용해 단어 간 관계를 파악합니다.  \\n'\n",
      " '지식이 눈덩이처럼 쌓이는 ‘스케일링’ 덕분에 모델 크기와 데이터가 커질수록 성능이 급격히 향상됩니다.  \\n'\n",
      " '이후 인간 피드백 강화학습(RLHF)으로 도움·무해·정직 등 가치를 반영해 답변 품질을 끌어올립니다.')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 3},\n",
    "    {\"model_name\": \"Gemini\", \"count\": 4},\n",
    "    {\"model_name\": \"claude\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    print(type(prompt), prompt)\n",
    "    response = llm.invoke(prompt)\n",
    "    pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) ChatPromptTemplate\n",
    "* Tuple 형태의 system, user, assistant 메시지 지원\n",
    "* 여러 개의 메시지를 조합하여 LLM에게 전달 가능\n",
    "* 간결성과 가독성이 높고 단순한 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='This system is an expert in answering questions about AI. Please provide clear and detailed explanations.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ChatGPT 모델의 학습 원리를 설명해 주세요.', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ChatGPT는 “대규모 언어 모델(Large Language Model, LLM)” 계열로, **‘다음 토큰 예측(next-token prediction)’**이라는 단순한 원리를 극단적으로 확장한 결과물입니다. 핵심 흐름은 (1) **사전학습(pre-training)** → (2) **지도미세조정(Supervised Fine-Tuning, SFT)** → (3) **강화학습 기반 인간 피드백 조정(RLHF)** 의 3단계로 설명할 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 사전학습(Pre-training) : “인터넷 글을 암기하다”\n",
      "- **목표**  \n",
      "  주어진 앞 문장(컨텍스트)를 보고 다음에 올 법한 토큰(일반적으로 1~4 글자 분량)을 확률적으로 맞추는 것.\n",
      "\n",
      "- **데이터**  \n",
      "  공공 웹, 도서, 위키, 깃허브 등 수천억 토큰 규모의 “정제되지 않은” 대규모 코퍼스.\n",
      "\n",
      "- **모델 구조**  \n",
      "  Transformer 디코더(예: GPT-3 175B → 96층, 96 head, 12888 차원 임베딩).  \n",
      "  셀프어텐션을 통해 임의 거리의 단어 관계를 계산.\n",
      "\n",
      "- **학습 방식**  \n",
      "  - 입력 시퀀스에 대해 각 위치의 정답 토큰에 대한 음의 로우 확률(negative log-likelihood)을 최소화  \n",
      "  - 배치 크기 3.2 M토큰, 3000억 토큰을 1~2 epoch 정도.  \n",
      "  - 수렴까지 V100/A100 GPU 수천 대, 수 주~수 개월 소요.\n",
      "\n",
      "- **결과**  \n",
      "  - 문법, 상식, 프로그래밍 언어, 외국어 등 다양한 패턴을 암기한 ‘기본 모델(Base Model)’이 됨.  \n",
      "  - 질문에 답하거나 코드를 생성할 수 있지만, “계속 써달라”식 완성 외에는 부적절한 응답이 많음.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 지도 미세조정(SFT) : “지시-응답 예시를 외우다”\n",
      "- **목표**  \n",
      "  사람이 작성한 ‘지시문(prompt) + 적절한 답변’ 쌍을 학습시켜, 지시 따르기(instruction following) 능력을 높임.\n",
      "\n",
      "- **데이터**  \n",
      "  1~10만 개 수준의 고품질 인간 작성 쌍(예: OpenAI의 InstructGPT/GPT-3.5 데이터, 최근엔 GPT-4 자체 생성 데이터 포함).\n",
      "\n",
      "- **학습 방식**  \n",
      "  동일한 next-token loss를 쓰지만, Base 모델에 비해 ‘좋은’ 분포만 반복적으로 노출 → 희망하는 스타일(안전, 간결, 사실 기반 등)로 출력 확률이 높아짐.\n",
      "\n",
      "- **결과**  \n",
      "  “다음 글을 요약해줘” 같은 형식의 지시를 이해하고, 안전·도덕성 필터링된 응답을 만들기 시작.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. RLHF(강화학습 기반 인간 피드백 조정) : “인간이 선호하는 답변을 확률적으로 끌어올리기”\n",
      "- **요약**  \n",
      "  “좋은 답” vs “나쁜 답”을 사람이 비교한 데이터 → 보상 모델(Reward Model, RM) 학습 → PPO 같은 강화학습으로 SFT 모델 파라미터 조정.\n",
      "\n",
      "- **세부 단계**  \n",
      "  1) **보상 모델 학습**  \n",
      "     동일 프롬프트에 대해 4~9개 응답을 만들고 사람이 순위 매김 → Bradley-Terry 모델로 RM 훈련.  \n",
      "  2) **정책 최적화(PPO)**  \n",
      "     RM이 매긴 점수를 보상으로, KL-페널티를 걸어 원래 SFT 모델과 너무 멀어지지 않도록 제약.  \n",
      "  3) **반복 개선**  \n",
      "     새로운 정책에서 샘플링 → RM 데이터 증강 → RM 재학습 → PPO 재적용.\n",
      "\n",
      "- **효과**  \n",
      "  - 헛소리(환각) 감소, 유해·차별적 표현 억제, 도움되는 길이/형식 선호.  \n",
      "  - “거절하기” 스타일(윤리적·정책적 한계) 확립.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. 추가 팁: 왜 “대화”처럼 보이나?\n",
      "- **컨텍스트 길이 확장**  \n",
      "  GPT-3.5 → 4K, GPT-4-turbo → 128K 토큰. 이전 대화 기록을 그대로 넣어 “기억” 흉내.\n",
      "\n",
      "- **시스템 메시지**  \n",
      "  프롬프트 최상단에 “당신은 도우미입니다. …”라는 프리앰블로 역할을 고정.\n",
      "\n",
      "- **추론 시 확률 조절**  \n",
      "  temperature, top-p 샘플링으로 창의성/일관성 균형.\n",
      "\n",
      "- **모델 크기 ≠ 성능 전부**  \n",
      "  Instruction + SFT + RLHF가 없으면 100B 모델도 “다음 문장”만 잘 맞추는 ‘글쓰기 도구’에 불과.\n",
      "\n",
      "---\n",
      "\n",
      "### 핵심 요약\n",
      "1. 방대한 웹 텍스트로 “다음 단어 맞추기”를 학습해 기초 언어 모델을 만든다.  \n",
      "2. 사람이 작성한 ‘지시↔좋은 답변’ 예시만 반복 학습해 지시 따르기 능력을 붙인다(SFT).  \n",
      "3. 사람이 선호하는 순위 데이터로 보상 모델을 만들고, 강화학습(PPO)으로 응답 품질을 최적화한다(RLHF).\n",
      "\n",
      "이 3단계를 통해 “단순 확률 기계”였던 모델은 ‘대화형 AI’로 변신하며, 질문에 답하고, 윤리적 한계를 지키고, 사람이 선호하는 스타일로 출력할 수 있게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    # role, message\n",
    "    (\"system\", \"This system is an expert in answering questions about {topic}. Please provide clear and detailed explanations.\"),\n",
    "    (\"human\", \"{model_name} 모델의 학습 원리를 설명해 주세요.\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", model_name=\"ChatGPT\")\n",
    "print(messages)\n",
    "\n",
    "# 생성한 메시지를 바로 주입하여 호출하기\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ChatGPT는 “만들어진 뒤엔 단순한 자동완성기”지만, 그 자동완성 능력을 얻기까지 3단계의 학습 절차를 거칩니다. 각 단계에서 사용되는 데이터, 목표, 최적화 기법이 다르며, 이를 이해하면 “왜 ChatGPT가 대화를 할 수 있는지”를 설명할 수 있습니다. 아래는 2024년 4월 기준으로 공개된 정보만을 정리한 설명입니다.\n",
      "\n",
      "----------------------------------------\n",
      "1. 단계 0: 사전학습(Pre-training)  \n",
      "   목표 → “세상에 존재하는 텍스트를 그대로 압축”  \n",
      "   데이터 → 공개 웹, 도서, 코드 등 수천 어휘 토큰(약 10 TB 급)  \n",
      "   방법 → “다음 토큰 예측”을 수식화한 최대가능도 추정(Maximum Likelihood)  \n",
      "   결과물 → GPT-베이스(base) 모델  \n",
      "   핵심 포인트  \n",
      "   - Transformer의 디코더만 쌓은 ‘언어 모델’이며, 프롬프트가 주어지면 확률적으로 토큰을 이어 씀.  \n",
      "   - 지식이 들어 있지만 “사람이 원하는 방식”으로 답을 하지는 않음(홀루시네이션, 편향, 안전성 미확보).  \n",
      "   - 이 시점엔 “대화” 데이터는 거의 없으므로, 사용자 질문에 “왜 그런지 설명해줘”라고 해도 질문 자체를 계속 이어 쓰기만 함.\n",
      "\n",
      "----------------------------------------\n",
      "2. 단계 1: 지도 파인튜닝(Supervised Fine-Tuning, SFT)  \n",
      "   목표 → “대화 형식을 익히고, 지시사항을 따르는 습관 기르기”  \n",
      "   데이터 → 대화·지시·응답 쌍 10~100k 수준(품질 높은 ‘교사 데이터’)  \n",
      "   방법 → 동일한 최대가능도 학습, 다만 레이블=‘올바른 응답’인 지시응답 쌍  \n",
      "   결과물 → SFT 모델(‘GPT-지시’라고도 불림)  \n",
      "   핵심 포인트  \n",
      "   - “프롬프트: A, 응답: B” 형태를 반복 학습해, 모델이 “지시를 끝내고 응답을 시작”하는 패턴을 익힘.  \n",
      "   - 여전히 “언제든 잘못된 답을 생성”할 수 있음. 안전성·진실성 확보는 못 함.\n",
      "\n",
      "----------------------------------------\n",
      "3. 단계 2: 강화학습(RLHF, Reinforcement Learning from Human Feedback)  \n",
      "   목표 → “인간이 ‘더 낫다’고 느끼는 답변을 확률적으로 증가”  \n",
      "   구성요소  \n",
      "   a) 보상 모델(Reward Model, RM)  \n",
      "      - SFT 모델이 생성한 답변 여러 개를 사람이 순위 매김(≈70k 개)  \n",
      "      - RM은 “주어진 프롬프트-답변 쌍에 대한 점수”를 예측하도록 회귀학습  \n",
      "   b) 정책(policy) = RLHF 대상 모델  \n",
      "      - PPO(Proximal Policy Optimization)로 RM 점수↑, KL-정규리즘(기존 SFT 모델과 너무 멀어지지 않도록)↓  \n",
      "   결과물 → 최종 ChatGPT 모델  \n",
      "   핵심 포\n"
     ]
    }
   ],
   "source": [
    "# 체인을 생성하여 호출하기\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\":\"AI\", \"model_name\":\"ChatGPT\"})\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ChatPromptTemplate\n",
    "* SystemMessagePromptTemplate와 HumanMessagePromptTemplate 클래스 사용\n",
    "* 객체 지향적 접근 - Message 객체를 독립적으로 생성 가능\n",
    "* 여러 조건에 따라 다른 시스템 메시지 선택\n",
    "\n",
    "```python\n",
    "if user_is_beginner:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"초보자를 위한 설명: {topic}\")\n",
    "else:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"전문가를 위한 상세 분석: {topic}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ChatMessagePromptTemplate 활용\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI expert in {topic}. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"What is deep learning?\")\n",
    "\n",
    "# LLM 호출\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatMessagePromptTemplate는 여러 종류의 메시지(시스템, 인간, AI)를 조합하여 복잡한 프롬프트를 생성할 때 유용합니다.\n",
    "* SystemMessagePromptTemplate: 이 템플릿은 AI 모델에게 역할을 부여하거나 전반적인 규칙을 설정하는 시스템 메시지를 만듭니다. 위의 예시에서는 \"번역을 도와주는 유용한 도우미\"라는 역할을 지정합니다.\n",
    "* HumanMessagePromptTemplate: 이 템플릿은 사용자의 질문이나 요청을 담는 인간 메시지를 만듭니다. 아래의 예시에서는 번역할 텍스트를 입력받습니다.\n",
    "* ChatPromptTemplate.from_messages: 이 클래스 메서드는 시스템 메시지, 인간 메시지 등 여러 종류의 MessagePromptTemplate 객체들을 리스트로 받아 하나의 채팅 프롬프트 템플릿으로 통합합니다.\n",
    "* format_messages: 이 메서드는 정의된 템플릿에 실제 값을 채워 넣어 [SystemMessage, HumanMessage] 형태의 리스트를 반환합니다. 이 리스트는 채팅 모델(Chat Model) 에 바로 전달될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant that translates English to Korean.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n",
      "나는 프로그래밍을 사랑해.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# 1. SystemMessagePromptTemplate와 HumanMessagePromptTemplate 생성\n",
    "# SystemMessagePromptTemplate는 모델의 페르소나 또는 기본 지침을 설정합니다.\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# HumanMessagePromptTemplate는 사용자로부터 받는 입력 프롬프트를 정의합니다.\n",
    "human_template = \"{text_to_translate}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 2. ChatPromptTemplate 생성\n",
    "# 위에서 만든 두 템플릿을 리스트로 묶어 ChatPromptTemplate을 만듭니다.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 3. 프롬프트 포맷팅\n",
    "# chat_prompt_template.format_messages()를 사용하여 최종 메시지 리스트를 생성합니다.\n",
    "# 이 함수는 딕셔너리 형태의 입력 변수를 받습니다.\n",
    "formatted_prompt = chat_prompt_template.format_messages(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"Korean\",\n",
    "    text_to_translate=\"I love programming.\"\n",
    ")\n",
    "\n",
    "# 4. 결과 출력\n",
    "print(formatted_prompt)\n",
    "\n",
    "# LLM 호출\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) FewShotPromptTemplate\n",
    "* FewShotPromptTemplate은 모델이 특정 형식을 따르게 하거나, 일관된 응답을 생성하도록 유도할 때 유용합니다.\n",
    "* 도메인 지식이 필요하거나, AI가 오답을 줄이고 더 신뢰할 만한 답변을 생성하도록 해야 할 때 효과적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1) PromptTemplate을 사용하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "태양계 행성을 “바위”와 “가스” 두 덩어리로 나눠 한 줄씩 정리하면 다음과 같습니다.\n",
      "\n",
      "수성 · 금성 · 지구 · 화성  \n",
      "땅덩어리(암행성)이고, 꼬리별처럼 얇은 대기만 씌워져 있다.\n",
      "\n",
      "목성 · 토성 · 천왕성 · 해왕성  \n",
      "거대 가스덩어리(목행성)로, 고리·위성·극광 등 장신구가 화려하다.\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate을 사용하지 않는 경우\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# chain 실행\n",
    "result = llm.invoke(\"태양계의 행성들을 간략히 정리해 주세요.\")\n",
    "\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-2) FewShotChatMessagePromptTemplate 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 초등학생도 쉽게 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': '뉴턴의 운동 법칙을 요약해 주세요.', 'output': '### 뉴턴의 운동 법칙\\n1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\\n2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\\n3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.'}, {'input': '지구의 대기 구성 요소를 알려주세요.', 'output': '### 지구 대기의 구성\\n- **질소 (78%)**: 대기의 대부분을 차지합니다.\\n- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\\n- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\\n- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000269CB831940>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000269CB7DAE70>, root_client=<openai.OpenAI object at 0x00000269CB85A930>, root_async_client=<openai.AsyncOpenAI object at 0x00000269CB85A960>, model_name='openai/gpt-oss-120b', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "## 태양계의 행성 8개 (간단 정리)\n",
      "\n",
      "| 순서(태양에서 멀어지는 순서) | 행성 | 종류 | 특징 (쉽게 말하면) |\n",
      "|---|---|---|---|\n",
      "| 1 | **수성** | 암석 행성 | 태양에 가장 가까워서 매우 뜨겁고, 밤에는 아주 추워요. |\n",
      "| 2 | **금성** | 암석 행성 | “지구의 쌍둥이”라고 불리지만, 대기가 두껍고 온도가 400 °C 이상이라 아주 뜨거워요. |\n",
      "| 3 | **지구** | 암석 행성 | 물과 생명이 있는 유일한 행성! 대기의 78%는 질소, 21%는 산소예요. |\n",
      "| 4 | **화성** | 암석 행성 | 붉은 색을 띠고, 옛날에 물이 있었을 가능성이 있어요. 현재는 얇은 이산화탄소 대기만 있어요. |\n",
      "| 5 | **목성** | 가스 행성 | 가장 큰 행성. 빨간 색 점(대적점)은 거대한 폭풍이에요. 79개의 위성을 가지고 있답니다. |\n",
      "| 6 | **토성** | 가스 행성 | 아름다운 고리(얼음과 바위 조각)로 유명해요. 고리 안쪽에 작은 위성도 많아요. |\n",
      "| 7 | **천왕성** | 얼음·가스 행성 | 자전축이 거의 옆으로 기울어져 있어, ‘옆으로 누운 행성’이라고 불려요. 푸른 색은 메탄 가스 때문이에요. |\n",
      "| 8 | **해왕성** | 얼음·가스 행성 | 가장 바깥쪽 행성. 바람이 아주 빠르게 불고, 푸른 색은 메탄 때문에 나타나요. |\n",
      "\n",
      "### 기억하면 좋은 포인트\n",
      "- **내행성(암석 행성)**: 수성, 금성, 지구, 화성 → 크기가 작고 표면이 바위로 이루어짐.  \n",
      "- **외행성(가스·얼음 행성)**: 목성, 토성, 천왕성, 해왕성 → 크기가 크고 대부분이 가스와 얼음으로 이루어짐.  \n",
      "- **태양계는 8개의 행성** (2006년 국제천문연맹이 명왕성을 ‘왜소 행성’으로 재분류하면서 8개가 됐어요).  \n",
      "\n",
      "이렇게 태양을 중심으로 8개의 행성이 차례대로 둥글게 돌고 있답니다! 🚀🌞\n"
     ]
    }
   ],
   "source": [
    "# FewShotChatMessagePromptTemplate 사용하는 경우\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 쉽게 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "chain = final_prompt | llm\n",
    "print(chain)\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"태양계의 행성들을 간략히 정리해 주세요.\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-1) PartialPrompt \n",
    "* 프롬프트를 더 동적으로 활용할 수 있으며, AI 응답을 더 일관성 있게 조정 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 프롬프트: 가을에 일어나는 대표적인 지구과학 현상은 태풍 발생이 맞나요? 가을에 주로 발생하는 지구과학 현상을 3개 알려주세요\n",
      " 모델 응답: 가을에 일어나는 대표적인 지구과학 현상으로 **태풍(열대저기압)**을 꼽을 수 있습니다. 한국을 비롯한 동아시아 지역에서는 8월 말부터 10월 초까지 태풍이 가장 많이 발생하고, 가을이 될수록 남쪽에서 북쪽으로 이동하면서 한반도에 영향을 미치는 경우가 많기 때문입니다. 다만 “가을에만” 일어나는 현상은 아니며, 다른 계절에도 나타날 수 있는 현상과 겹치는 부분이 있습니다.  \n",
      "\n",
      "아래는 **가을에 특히 활발히 나타나는 주요 지구과학 현상 3가지**입니다.\n",
      "\n",
      "| 현상 | 주요 특징 및 원인 | 가을에 두드러지는 이유 |\n",
      "|------|-------------------|------------------------|\n",
      "| **1. 태풍·열대저기압** | 바다 위에서 형성된 저기압이 강한 바람·폭우·해일을 동반함. 남서쪽의 온난·다습한 해수와 대기 불안정이 필요. | 가을이 되면 서쪽 태평양의 해수면 온도가 아직 높아 태풍이 강하게 발달하고, 제트기류가 남쪽으로 내려와 태풍 경로를 한반도·동아시아로 끌어당깁니다. |\n",
      "| **2. 가을철 대기 역전·연무(안개)·연무 현상** | 지표면이 빠르게 냉각되면서 차가운 공기가 지표면에 머물고, 위쪽에 따뜻한 공층이 형성돼 대기 역전이 발생. 수증기가 응결해 안개·연무가 자주 나타남. | 가을은 일교차가 크게 늘고, 밤에 급격히 냉각되는 경우가 많아 대기 역전이 쉽게 형성됩니다. 특히 강원도·내륙 지방에서 산악 안개, 평야에서는 연무가 자주 보입니다. |\n",
      "| **3. 가을 강수·전선 활동 강화** | 한반도는 대륙성 고기압(시베리아 고기압)과 남쪽의 남동아시아 저기압이 교차하면서 전선이 활발히 움직임. 이때 강수와 급격한 기온 변화가 일어남. | 가을에 시베리아 고기압이 남하하면서 차가운 북풍이 남쪽의 온난·다습한 공기와 만나 전선을 형성합니다. 전선이 통과할 때는 비·눈·우박이 동반되는 강수가 자주 발생합니다. |\n",
      "\n",
      "### 간단히 정리하면\n",
      "\n",
      "1. **태풍** – 가을에 가장 활발히 발생하고, 한반도에 큰 영향을 미치는 대표적인 현상.  \n",
      "2. **대기 역전·안개·연무** – 급격한 일교차와 차가운 지표면 때문에 가을에 흔히 나타남.  \n",
      "3. **가을 강수·전선 활동** – 시베리아 고기압과 남동아시아 저기압의 교차로 인해 강수와 급격한 기온 변화가 빈번.\n",
      "\n",
      "이 세 가지 현상은 모두 **가을에 특히 두드러지지만**, 다른 계절에도 어느 정도 나타날 수 있다는 점을 기억하면 좋습니다. 필요에 따라 더 자세한 메커니즘이나 지역별 특성을 알려드릴 수도 있습니다. 언제든 궁금한 점 있으면 질문 주세요!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 계절을 결정하는 함수 (남반구/북반구 고려)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # 북반구 (기본값)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"봄\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"여름\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"가을\"\n",
    "        else:\n",
    "            return \"겨울\"\n",
    "    else:  # 남반구 (계절 반대)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"가을\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"겨울\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"봄\"\n",
    "        else:\n",
    "            return \"여름\"\n",
    "\n",
    "# 프롬프트 템플릿 정의 (부분 변수 적용)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}에 일어나는 대표적인 지구과학 현상은 {phenomenon}이 맞나요? {season}에 주로 발생하는 지구과학 현상을 3개 알려주세요\",\n",
    "    input_variables=[\"phenomenon\"],  # 사용자 입력 필요\n",
    "    partial_variables={\"season\": get_current_season()}  # 동적으로 계절 값 할당\n",
    ")\n",
    "\n",
    "# OpenAI 모델 초기화\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# 특정 계절의 현상 질의\n",
    "query = prompt.format(phenomenon=\"태풍 발생\")\n",
    "result = llm.invoke(query)\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(f\" 프롬프트: {query}\")\n",
    "print(f\" 모델 응답: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 계절: 가을\n",
      "\n",
      " 가을에 발생하는 자연 현상:\n",
      "1.  **성운**: 가을 밤에 밝은 별자리인 페가수스자리 근처에서 관찰할 수 있는 성운은 가을의 대표적인 천문 현상 중 하나입니다. 이 시기에 공기가 맑고 추운 날씨로 인해 밤하늘에서 더 많은 별을 볼 수 있습니다. 이 시기에는 성운이 선명하게 나타날 때가 많습니다.\n",
      "2.  **수확월**: 지구가 태양을 중심으로 회전하면서 가을에는 밤과 낮의 길이가 달라지기 때문에 낮이 짧아지고 밤이 길어집니다. 이로 인해 기온이 더욱 낮아지고 추운 날씨가 시작됩니다. 이 시기에 농작물이 익고 수확할 수 있습니다.\n",
      "3.  **가을 단풍**: 가을에는 일교차로 인해 밤과 낮의 기온이 크게 달라집니다. 이러한 기온 변화로 인해 나무의 잎이 색이 변하고 예쁜 단풍이 나타납니다. 붉은색, 노란색, 주황색 등 다양한 색깔의 단풍이 계절의 변화를 나타냅니다.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 계절을 결정하는 함수 (남반구/북반구 고려)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # 북반구 (기본값)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"봄\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"여름\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"가을\"\n",
    "        else:\n",
    "            return \"겨울\"\n",
    "    else:  # 남반구 (계절 반대)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"가을\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"겨울\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"봄\"\n",
    "        else:\n",
    "            return \"여름\"\n",
    "\n",
    "# Step 1: 현재 계절 결정\n",
    "season = get_current_season(\"north\")  # 계절 값 얻기\n",
    "print(f\"현재 계절: {season}\")\n",
    "\n",
    "# Step 2: 해당 계절의 자연 현상 추천\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{season}에 주로 발생하는 대표적인 지구과학 현상 3가지를 알려주세요. \"\n",
    "    \"각 현상에 대해 간단한 설명을 포함해주세요.\"\n",
    ")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# 체인 2: 자연 현상 추천 (입력: 계절 → 출력: 자연 현상 목록)\n",
    "chain2 = (\n",
    "    {\"season\": lambda x : season}  # chain1의 출력을 season 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행: 현재 계절에 따른 자연 현상 추천\n",
    "response = chain2.invoke({})\n",
    "print(f\"\\n {season}에 발생하는 자연 현상:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'season_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     17\u001b[39m chain2 = (\n\u001b[32m     18\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x : season_name}  \u001b[38;5;66;03m# chain1의 출력을 season 변수로 전달\u001b[39;00m\n\u001b[32m     19\u001b[39m     | prompt2\n\u001b[32m     20\u001b[39m     | llm\n\u001b[32m     21\u001b[39m     | StrOutputParser()\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 실행: 현재 계절에 따른 자연 현상 추천\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m response = \u001b[43mchain2\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m에 발생하는 자연 현상:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3243\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3241\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3243\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3244\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3245\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4000\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3995\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3996\u001b[39m         futures = [\n\u001b[32m   3997\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3998\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3999\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m4000\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   4001\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   4002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3984\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3978\u001b[39m child_config = patch_config(\n\u001b[32m   3979\u001b[39m     config,\n\u001b[32m   3980\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3981\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3982\u001b[39m )\n\u001b[32m   3983\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3984\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3986\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5024\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this ``Runnable`` synchronously.\u001b[39;00m\n\u001b[32m   5010\u001b[39m \n\u001b[32m   5011\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5021\u001b[39m \n\u001b[32m   5022\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m5024\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5025\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5026\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5028\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5030\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5031\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2089\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2085\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2086\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2087\u001b[39m         output = cast(\n\u001b[32m   2088\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2089\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2091\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2093\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2094\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2096\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2097\u001b[39m         )\n\u001b[32m   2098\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2099\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4881\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4879\u001b[39m                 output = chunk\n\u001b[32m   4880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4881\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      9\u001b[39m llm = ChatOpenAI(\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m#api_key=OPENAI_API_KEY,\u001b[39;00m\n\u001b[32m     11\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.groq.com/openai/v1\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Groq API 엔드포인트\u001b[39;00m\n\u001b[32m     12\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/llama-4-scout-17b-16e-instruct\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Spring AI와 동일한 모델\u001b[39;00m\n\u001b[32m     13\u001b[39m     temperature=\u001b[32m0.0\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 체인 2: 자연 현상 추천 (입력: 계절 → 출력: 자연 현상 목록)\u001b[39;00m\n\u001b[32m     17\u001b[39m chain2 = (\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mseason_name\u001b[49m}  \u001b[38;5;66;03m# chain1의 출력을 season 변수로 전달\u001b[39;00m\n\u001b[32m     19\u001b[39m     | prompt2\n\u001b[32m     20\u001b[39m     | llm\n\u001b[32m     21\u001b[39m     | StrOutputParser()\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 실행: 현재 계절에 따른 자연 현상 추천\u001b[39;00m\n\u001b[32m     25\u001b[39m response = chain2.invoke({})\n",
      "\u001b[31mNameError\u001b[39m: name 'season_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2: 해당 계절의 자연 현상 추천\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{season}에 주로 발생하는 대표적인 지구과학 현상 3가지를 알려주세요. \"\n",
    "    \"각 현상에 대해 간단한 설명을 포함해주세요.\"\n",
    ")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# 체인 2: 자연 현상 추천 (입력: 계절 → 출력: 자연 현상 목록)\n",
    "chain2 = (\n",
    "    {\"season\": lambda x : season_name}  # chain1의 출력을 season 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행: 현재 계절에 따른 자연 현상 추천\n",
    "response = chain2.invoke({})\n",
    "print(f\"\\n {season_name}에 발생하는 자연 현상:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2) PartialPrompt \n",
    "* API 호출 데이터, 시간 정보, 사용자 정보 등을 반영할 때 매우 유용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 프롬프트: 현재 1달러 = 1377.98원 기준으로 환율 정보를 알려드립니다. 이에 대한 분석을 제공해 주세요.\n",
      " 모델 응답: ## **2024년 4월 5일 환율 분석**\n",
      "\n",
      "### **1. 현재 환율: 1달러 = 1377.98원**\n",
      "\n",
      "2024년 4월 5일, 1달러의 가치는 1377.98원으로 평가되고 있습니다. 이는 원화 가치의 약세를 의미합니다. 아래는 다양한 측면에서 이 환율에 대한 분석입니다.\n",
      "\n",
      "### **2. 경제 지표 분석**\n",
      "\n",
      "- **글로벌 경제 상황:** 최근 달러화 강세의 주된 요인 중 하나는 미국 경제의 견조한 성장세와 인플레이션 압력입니다. 미국의 높은 금리 수준이 투자자들에게 매력적인 수익을 제공하고 있어 달러에 대한 수요가 증가하고 있습니다.\n",
      "\n",
      "- **한국 경제:** 한국 경제는 대외 무역 의존도가 높기 때문에 환율 변동에 민감합니다. 수출 중심의 경제 구조로 인해 약한 원화가 수출에 긍정적인 영향을 줄 수 있지만, 수입 물가의 상승으로 이어져 국내 인플레이션을 자극할 수 있습니다.\n",
      "\n",
      "### **3. 정책적 요인**\n",
      "\n",
      "- **한국은행:** 한국은행은 최근 금리 동결을 결정하는 등 고물가와 경기 둔화 우려 속에서 통화정책을 신중하게 운영하고 있습니다. 추가적인 금리 인상이나 인하 여부는 경제 상황에 따라 달라질 것입니다.\n",
      "\n",
      "- **FED(연준):** 미국 연방준비제도(FED)는 인플레이션 목표 달성을 위해 금리를 조정하고 있습니다. 향후 금리 정책이 달러 가치에 영향을 미칠 것입니다.\n",
      "\n",
      "### **4. 시장 전망**\n",
      "\n",
      "- **단기 전망:** 단기적으로는 글로벌 경제 상황과 주요 경제국의 통화 정책에 따라 환율이 변동할 것입니다. 지정학적 리스크나 무역 관련 이슈도 환율에 영향을 줄 수 있습니다.\n",
      "\n",
      "- **장기 전망:** 장기적으로는 한국 경제의 펀더멘털과 글로벌 경제의 흐름이 중요합니다. 원화 가치의 안정화 여부는 인플레이션 관리, 무역 수지, 외국인 투자 유치 등에 달려 있습니다.\n",
      "\n",
      "### **5. 개인 및 기업에 미치는 영향**\n",
      "\n",
      "- **수출입 기업:** 원화 약세는 수출입 기업에 혼합된 영향을 미칩니다. 수출을 촉진할 수 있지만, 수입 원자재 가격 상승으로 비용 부담이 증가할 수 있습니다.\n",
      "\n",
      "- **여행객 및 해외 투자:** 개인에게는 해외여행 비용 증가, 해외 자산 가치 변동 등 다양한 영향을 미칩니다.\n",
      "\n",
      "### **6. 결론**\n",
      "\n",
      "현재의 환율은 다양한 경제적 요인과 정책적 결정에 의해 영향을 받고 있습니다. 향후 경제 상황과 정책 변화를 주기적으로 모니터링하여 전략을 수립하는 것이 중요합니다. 개인과 기업은 환율 변동에 따른 위험 관리와 기회 포착을 위해 전문적인 분석과 전략적 계획이 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 실시간 환율을 가져오는 함수\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1달러 = {data['rates']['KRW']}원\"\n",
    "\n",
    "# Partial Prompt 활용\n",
    "prompt = PromptTemplate(\n",
    "    template=\"현재 {info} 기준으로 환율 정보를 알려드립니다. 이에 대한 분석을 제공해 주세요.\",\n",
    "    input_variables=[],  # 사용자 입력 없음\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # API에서 가져온 데이터 자동 반영\n",
    ")\n",
    "\n",
    "# LLM 모델 설정\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# 모델에 프롬프트 전달 및 응답 받기\n",
    "response = llm.invoke(prompt.format())\n",
    "\n",
    "# 결과 출력\n",
    "print(\" 프롬프트:\", prompt.format())\n",
    "print(\" 모델 응답:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate \n",
    "* [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate)\n",
    "* [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)\n",
    "* [ChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html#langchain_core.prompts.chat.ChatMessagePromptTemplate)\n",
    "* [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html#langchain_core.prompts.few_shot.FewShotPromptTemplate)\n",
    "* PartialPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_6\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) PromptTemplate ì˜ from_template() í•¨ìˆ˜ ì‚¬ìš©\n",
    "* ì£¼ë¡œ LLM(í…ìŠ¤íŠ¸ ì™„ì„±í˜• ëª¨ë¸, ex. Ollama, GPT-3.5)ê³¼ í•¨ê»˜ ì‚¬ìš©\n",
    "* í•˜ë‚˜ì˜ ë¬¸ìì—´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ChatGPTëŠ” ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ì‚¬ì „ í›ˆë ¨(preâ€‘training) ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. '\n",
      " 'ì´ ê³¼ì •ì—ì„œ Transformer êµ¬ì¡°ì™€ ìê¸°â€‘ì£¼ì˜(selfâ€‘attention) ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•´ ë¬¸ë§¥ì„ ì´í•´í•˜ê³  ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ '\n",
      " 'í†µí•©í•©ë‹ˆë‹¤. ì´í›„ ì¸ê°„ í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°•í™” í•™ìŠµ(RLHF) ë“±ì„ í†µí•´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ë³´ë‹¤ ì•ˆì „í•˜ê³  ìœ ìš©í•œ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ '\n",
      " 'ìµœì í™”í•©ë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pprint import pprint\n",
    "\n",
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    # model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) PromptTemplate ê²°í•©í•˜ê¸°\n",
    "* ë™ì¼í•œ Prompt íŒ¨í„´ì„ ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ëŸ¬ ê°œì˜ ì§ˆë¬¸ì„ ì‘ì„±í•´ì„œ LLMì„ ì‹¤í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['count', 'language', 'model_name'] input_types={} partial_variables={} template='{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.'\n",
      "('ChatGPTëŠ” ì¸í„°ë„·ì˜ ë°©ëŒ€í•œ ê¸€ì„ ë¯¸ë¦¬ í•™ìŠµí•´ ë‘ê³ , ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥ ë‹¤ìŒì— ì˜¬ ë²•í•œ ë‹¨ì–´ë“¤ì˜ í™•ë¥ ì„ ê³„ì‚°í•´ ë¬¸ì¥ì„ ë§Œë“¤ì–´ '\n",
      " 'ëƒ…ë‹ˆë‹¤.  \\n'\n",
      " 'í•™ìŠµ ë•ŒëŠ” â€œë§ˆìŠ¤í¬â€ë¡œ ì¼ë¶€ ë‹¨ì–´ë¥¼ ê°€ë ¤ ë†“ê³  ì´ë¥¼ ë§ì¶”ëŠ” ë°©ì‹ìœ¼ë¡œ ì–¸ì–´ íŒ¨í„´ì„ ìµíˆë©°, ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ íŠ¸ëœìŠ¤í¬ë¨¸ë¼ëŠ” êµ¬ì¡°ë¥¼ '\n",
      " 'ë°˜ë³µ í™œìš©í•©ë‹ˆë‹¤.  \\n'\n",
      " 'ì´í›„ ì‚¬ëŒì´ ì§ì ‘ ëŒ€í™” í’ˆì§ˆì„ í‰ê°€í•´ ì£¼ëŠ” â€œê°•í™”í•™ìŠµâ€ì„ ê±°ì¹˜ë©´ì„œ ë” ìì—°ìŠ¤ëŸ½ê³  ìœ ìš©í•œ ë‹µë³€ì„ ë§Œë“¤ì–´ë‚´ë„ë¡ ë¯¸ì„¸ì¡°ì •ë©ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " 'ChatGPT ëª¨ë¸ì˜ ì¥ì  ìš”ì•½\\n'\n",
      " 'â€¢ ë¬¸ë§¥ì„ ê¸¸ê²Œ ê¸°ì–µí•´ ìì—°ìŠ¤ëŸ½ê³  ì—°ê²°ëœ ëŒ€í™” ê°€ëŠ¥  \\n'\n",
      " 'â€¢ ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ ë†’ì€ ì´í•´ë„ì™€ ë…¼ë¦¬ì  ë‹µë³€ ì œê³µ  \\n'\n",
      " 'â€¢ ì½”ë“œ ì‘ì„±, ë²ˆì—­, ìš”ì•½, ì•„ì´ë””ì–´ ìƒì„± ë“± í™œìš©ë„ê°€ ë›°ì–´ë‚¨  \\n'\n",
      " 'â€¢ ì‚¬ìš©ì ì˜ë„ì— ë”°ë¼ í†¤Â·ìŠ¤íƒ€ì¼ì„ ìœ ì—°í•˜ê²Œ ì¡°ì ˆ ê°€ëŠ¥  \\n'\n",
      " 'â€¢ ì§€ì†ì ì¸ ê°•í™”í•™ìŠµÂ·ì—…ë°ì´íŠ¸ë¡œ ì„±ëŠ¥ì´ ë¹ ë¥´ê²Œ ê°œì„ ë¨\\n'\n",
      " '\\n'\n",
      " 'ChatGPTì™€ ë¹„ìŠ·í•œ AI ëª¨ë¸\\n'\n",
      " 'â€¢ êµ¬ê¸€ ì œë¯¸ë‚˜ì´(Gemini)  \\n'\n",
      " 'â€¢ ë©”íƒ€ ë¼ë§ˆ(Llama)  \\n'\n",
      " 'â€¢ í´ë¼ìš°ë“œ ì•¤ì„œë¡œí”½ í´ë¼ìš°ë“œ(Claude)  \\n'\n",
      " 'â€¢ ì¹´ì¹´ì˜¤ ë¸Œë ˆì¸ ì½”ë„¥íŠ¸(KoGPT)')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# í…œí”Œë¦¿ì— ê°’ì„ ì±„ì›Œì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# ë¬¸ìì—´ í…œí”Œë¦¿ ê²°í•© (PromptTemplate + PromptTemplate + ë¬¸ìì—´)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\")\n",
    "              + \"\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"í•œêµ­ì–´\")\n",
    "print(combined_prompt)\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"í•œêµ­ì–´\"})\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°°ì—´ í˜•íƒœë¡œ í•˜ì—¬ ì—¬ëŸ¬ê°œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'Gemini ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'claude ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.']\n",
      "<class 'str'> GPT-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('GPT-4ëŠ” ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ì–´Â·ë¬¸ë§¥ ê°„ í™•ë¥  ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” â€˜Transformerâ€™ ì•„í‚¤í…ì²˜ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.  \\n'\n",
      " 'í•™ìŠµ ì‹œ ì…ë ¥ ë¬¸ì¥ì˜ ì¼ë¶€ë¥¼ ê°€ë¦¬ê³  ë‹¤ìŒ í† í°ì„ ë§íˆëŠ” â€˜ìê¸° ì§€ë„ í•™ìŠµâ€™ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ë‚´ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ë³µ ì¡°ì •í•©ë‹ˆë‹¤.  \\n'\n",
      " 'ì´ë ‡ê²Œ ì–»ì€ íŒ¨í„´ê³¼ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ì…ë ¥ì— ëŒ€í•´ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.')\n",
      "<class 'str'> Gemini ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('GeminiëŠ” ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸Â·ì´ë¯¸ì§€Â·ì˜¤ë””ì˜¤ ë“± ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°ë¥¼ ë™ì‹œì— í•™ìŠµí•´ íŒ¨í„´ì„ í¬ì°©í•©ë‹ˆë‹¤.  \\n'\n",
      " 'Transformer ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¡œ, ì…ë ¥ í† í° ê°„ ê´€ê³„ë¥¼ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ê³„ì‚°í•˜ë©° ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.  \\n'\n",
      " 'ê°•í™”í•™ìŠµê³¼ ì¸ê°„ í”¼ë“œë°±(RLHF)ìœ¼ë¡œ ì •ë‹µ ê²½í–¥ì„ ë°˜ì˜í•´ ë³´ìƒì„ ìµœëŒ€í™”í•˜ë©° ì •ë°€ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.  \\n'\n",
      " 'ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë¶„ì‚° ì²˜ë¦¬í•˜ë©°, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ê³¼ ì •ê·œí™”ë¡œ ê³¼ì í•©ì„ ì–µì œí•´ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.')\n",
      "<class 'str'> claude ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('í´ë¡œë“œëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ, ì¸í„°ë„·Â·ì±… ë“± ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ í†µí•´ ë‹¤ìŒ í† í°ì´ ë¬´ì—‡ì¼ì§€ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.  \\n'\n",
      " 'í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ë¬¸ë§¥ì„ ê¸°ì–µÂ·í™œìš©í•´ ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.  \\n'\n",
      " 'ì§€ì‹ì´ ëˆˆë©ì´ì²˜ëŸ¼ ìŒ“ì´ëŠ” â€˜ìŠ¤ì¼€ì¼ë§â€™ ë•ë¶„ì— ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ í–¥ìƒë©ë‹ˆë‹¤.  \\n'\n",
      " 'ì´í›„ ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ(RLHF)ìœ¼ë¡œ ë„ì›€Â·ë¬´í•´Â·ì •ì§ ë“± ê°€ì¹˜ë¥¼ ë°˜ì˜í•´ ë‹µë³€ í’ˆì§ˆì„ ëŒì–´ì˜¬ë¦½ë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 3},\n",
    "    {\"model_name\": \"Gemini\", \"count\": 4},\n",
    "    {\"model_name\": \"claude\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ ìƒì„±\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # ë¯¸ë¦¬ ìƒì„±ëœ ì§ˆë¬¸ ëª©ë¡ í™•ì¸\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    print(type(prompt), prompt)\n",
    "    response = llm.invoke(prompt)\n",
    "    pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) ChatPromptTemplate\n",
    "* Tuple í˜•íƒœì˜ system, user, assistant ë©”ì‹œì§€ ì§€ì›\n",
    "* ì—¬ëŸ¬ ê°œì˜ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬ ê°€ëŠ¥\n",
    "* ê°„ê²°ì„±ê³¼ ê°€ë…ì„±ì´ ë†’ê³  ë‹¨ìˆœí•œ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='This system is an expert in answering questions about AI. Please provide clear and detailed explanations.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ChatGPT ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ChatGPTëŠ” â€œëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(Large Language Model, LLM)â€ ê³„ì—´ë¡œ, **â€˜ë‹¤ìŒ í† í° ì˜ˆì¸¡(next-token prediction)â€™**ì´ë¼ëŠ” ë‹¨ìˆœí•œ ì›ë¦¬ë¥¼ ê·¹ë‹¨ì ìœ¼ë¡œ í™•ì¥í•œ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤. í•µì‹¬ íë¦„ì€ (1) **ì‚¬ì „í•™ìŠµ(pre-training)** â†’ (2) **ì§€ë„ë¯¸ì„¸ì¡°ì •(Supervised Fine-Tuning, SFT)** â†’ (3) **ê°•í™”í•™ìŠµ ê¸°ë°˜ ì¸ê°„ í”¼ë“œë°± ì¡°ì •(RLHF)** ì˜ 3ë‹¨ê³„ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. ì‚¬ì „í•™ìŠµ(Pre-training) : â€œì¸í„°ë„· ê¸€ì„ ì•”ê¸°í•˜ë‹¤â€\n",
      "- **ëª©í‘œ**  \n",
      "  ì£¼ì–´ì§„ ì• ë¬¸ì¥(ì»¨í…ìŠ¤íŠ¸)ë¥¼ ë³´ê³  ë‹¤ìŒì— ì˜¬ ë²•í•œ í† í°(ì¼ë°˜ì ìœ¼ë¡œ 1~4 ê¸€ì ë¶„ëŸ‰)ì„ í™•ë¥ ì ìœ¼ë¡œ ë§ì¶”ëŠ” ê²ƒ.\n",
      "\n",
      "- **ë°ì´í„°**  \n",
      "  ê³µê³µ ì›¹, ë„ì„œ, ìœ„í‚¤, ê¹ƒí—ˆë¸Œ ë“± ìˆ˜ì²œì–µ í† í° ê·œëª¨ì˜ â€œì •ì œë˜ì§€ ì•Šì€â€ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤.\n",
      "\n",
      "- **ëª¨ë¸ êµ¬ì¡°**  \n",
      "  Transformer ë””ì½”ë”(ì˜ˆ: GPT-3 175B â†’ 96ì¸µ, 96 head, 12888 ì°¨ì› ì„ë² ë”©).  \n",
      "  ì…€í”„ì–´í…ì…˜ì„ í†µí•´ ì„ì˜ ê±°ë¦¬ì˜ ë‹¨ì–´ ê´€ê³„ë¥¼ ê³„ì‚°.\n",
      "\n",
      "- **í•™ìŠµ ë°©ì‹**  \n",
      "  - ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ê° ìœ„ì¹˜ì˜ ì •ë‹µ í† í°ì— ëŒ€í•œ ìŒì˜ ë¡œìš° í™•ë¥ (negative log-likelihood)ì„ ìµœì†Œí™”  \n",
      "  - ë°°ì¹˜ í¬ê¸° 3.2 Mí† í°, 3000ì–µ í† í°ì„ 1~2 epoch ì •ë„.  \n",
      "  - ìˆ˜ë ´ê¹Œì§€ V100/A100 GPU ìˆ˜ì²œ ëŒ€, ìˆ˜ ì£¼~ìˆ˜ ê°œì›” ì†Œìš”.\n",
      "\n",
      "- **ê²°ê³¼**  \n",
      "  - ë¬¸ë²•, ìƒì‹, í”„ë¡œê·¸ë˜ë° ì–¸ì–´, ì™¸êµ­ì–´ ë“± ë‹¤ì–‘í•œ íŒ¨í„´ì„ ì•”ê¸°í•œ â€˜ê¸°ë³¸ ëª¨ë¸(Base Model)â€™ì´ ë¨.  \n",
      "  - ì§ˆë¬¸ì— ë‹µí•˜ê±°ë‚˜ ì½”ë“œë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, â€œê³„ì† ì¨ë‹¬ë¼â€ì‹ ì™„ì„± ì™¸ì—ëŠ” ë¶€ì ì ˆí•œ ì‘ë‹µì´ ë§ìŒ.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. ì§€ë„ ë¯¸ì„¸ì¡°ì •(SFT) : â€œì§€ì‹œ-ì‘ë‹µ ì˜ˆì‹œë¥¼ ì™¸ìš°ë‹¤â€\n",
      "- **ëª©í‘œ**  \n",
      "  ì‚¬ëŒì´ ì‘ì„±í•œ â€˜ì§€ì‹œë¬¸(prompt) + ì ì ˆí•œ ë‹µë³€â€™ ìŒì„ í•™ìŠµì‹œì¼œ, ì§€ì‹œ ë”°ë¥´ê¸°(instruction following) ëŠ¥ë ¥ì„ ë†’ì„.\n",
      "\n",
      "- **ë°ì´í„°**  \n",
      "  1~10ë§Œ ê°œ ìˆ˜ì¤€ì˜ ê³ í’ˆì§ˆ ì¸ê°„ ì‘ì„± ìŒ(ì˜ˆ: OpenAIì˜ InstructGPT/GPT-3.5 ë°ì´í„°, ìµœê·¼ì—” GPT-4 ìì²´ ìƒì„± ë°ì´í„° í¬í•¨).\n",
      "\n",
      "- **í•™ìŠµ ë°©ì‹**  \n",
      "  ë™ì¼í•œ next-token lossë¥¼ ì“°ì§€ë§Œ, Base ëª¨ë¸ì— ë¹„í•´ â€˜ì¢‹ì€â€™ ë¶„í¬ë§Œ ë°˜ë³µì ìœ¼ë¡œ ë…¸ì¶œ â†’ í¬ë§í•˜ëŠ” ìŠ¤íƒ€ì¼(ì•ˆì „, ê°„ê²°, ì‚¬ì‹¤ ê¸°ë°˜ ë“±)ë¡œ ì¶œë ¥ í™•ë¥ ì´ ë†’ì•„ì§.\n",
      "\n",
      "- **ê²°ê³¼**  \n",
      "  â€œë‹¤ìŒ ê¸€ì„ ìš”ì•½í•´ì¤˜â€ ê°™ì€ í˜•ì‹ì˜ ì§€ì‹œë¥¼ ì´í•´í•˜ê³ , ì•ˆì „Â·ë„ë•ì„± í•„í„°ë§ëœ ì‘ë‹µì„ ë§Œë“¤ê¸° ì‹œì‘.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. RLHF(ê°•í™”í•™ìŠµ ê¸°ë°˜ ì¸ê°„ í”¼ë“œë°± ì¡°ì •) : â€œì¸ê°„ì´ ì„ í˜¸í•˜ëŠ” ë‹µë³€ì„ í™•ë¥ ì ìœ¼ë¡œ ëŒì–´ì˜¬ë¦¬ê¸°â€\n",
      "- **ìš”ì•½**  \n",
      "  â€œì¢‹ì€ ë‹µâ€ vs â€œë‚˜ìœ ë‹µâ€ì„ ì‚¬ëŒì´ ë¹„êµí•œ ë°ì´í„° â†’ ë³´ìƒ ëª¨ë¸(Reward Model, RM) í•™ìŠµ â†’ PPO ê°™ì€ ê°•í™”í•™ìŠµìœ¼ë¡œ SFT ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì •.\n",
      "\n",
      "- **ì„¸ë¶€ ë‹¨ê³„**  \n",
      "  1) **ë³´ìƒ ëª¨ë¸ í•™ìŠµ**  \n",
      "     ë™ì¼ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ 4~9ê°œ ì‘ë‹µì„ ë§Œë“¤ê³  ì‚¬ëŒì´ ìˆœìœ„ ë§¤ê¹€ â†’ Bradley-Terry ëª¨ë¸ë¡œ RM í›ˆë ¨.  \n",
      "  2) **ì •ì±… ìµœì í™”(PPO)**  \n",
      "     RMì´ ë§¤ê¸´ ì ìˆ˜ë¥¼ ë³´ìƒìœ¼ë¡œ, KL-í˜ë„í‹°ë¥¼ ê±¸ì–´ ì›ë˜ SFT ëª¨ë¸ê³¼ ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šë„ë¡ ì œì•½.  \n",
      "  3) **ë°˜ë³µ ê°œì„ **  \n",
      "     ìƒˆë¡œìš´ ì •ì±…ì—ì„œ ìƒ˜í”Œë§ â†’ RM ë°ì´í„° ì¦ê°• â†’ RM ì¬í•™ìŠµ â†’ PPO ì¬ì ìš©.\n",
      "\n",
      "- **íš¨ê³¼**  \n",
      "  - í—›ì†Œë¦¬(í™˜ê°) ê°ì†Œ, ìœ í•´Â·ì°¨ë³„ì  í‘œí˜„ ì–µì œ, ë„ì›€ë˜ëŠ” ê¸¸ì´/í˜•ì‹ ì„ í˜¸.  \n",
      "  - â€œê±°ì ˆí•˜ê¸°â€ ìŠ¤íƒ€ì¼(ìœ¤ë¦¬ì Â·ì •ì±…ì  í•œê³„) í™•ë¦½.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. ì¶”ê°€ íŒ: ì™œ â€œëŒ€í™”â€ì²˜ëŸ¼ ë³´ì´ë‚˜?\n",
      "- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¥**  \n",
      "  GPT-3.5 â†’ 4K, GPT-4-turbo â†’ 128K í† í°. ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ê·¸ëŒ€ë¡œ ë„£ì–´ â€œê¸°ì–µâ€ í‰ë‚´.\n",
      "\n",
      "- **ì‹œìŠ¤í…œ ë©”ì‹œì§€**  \n",
      "  í”„ë¡¬í”„íŠ¸ ìµœìƒë‹¨ì— â€œë‹¹ì‹ ì€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. â€¦â€ë¼ëŠ” í”„ë¦¬ì•°ë¸”ë¡œ ì—­í• ì„ ê³ ì •.\n",
      "\n",
      "- **ì¶”ë¡  ì‹œ í™•ë¥  ì¡°ì ˆ**  \n",
      "  temperature, top-p ìƒ˜í”Œë§ìœ¼ë¡œ ì°½ì˜ì„±/ì¼ê´€ì„± ê· í˜•.\n",
      "\n",
      "- **ëª¨ë¸ í¬ê¸° â‰  ì„±ëŠ¥ ì „ë¶€**  \n",
      "  Instruction + SFT + RLHFê°€ ì—†ìœ¼ë©´ 100B ëª¨ë¸ë„ â€œë‹¤ìŒ ë¬¸ì¥â€ë§Œ ì˜ ë§ì¶”ëŠ” â€˜ê¸€ì“°ê¸° ë„êµ¬â€™ì— ë¶ˆê³¼.\n",
      "\n",
      "---\n",
      "\n",
      "### í•µì‹¬ ìš”ì•½\n",
      "1. ë°©ëŒ€í•œ ì›¹ í…ìŠ¤íŠ¸ë¡œ â€œë‹¤ìŒ ë‹¨ì–´ ë§ì¶”ê¸°â€ë¥¼ í•™ìŠµí•´ ê¸°ì´ˆ ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“ ë‹¤.  \n",
      "2. ì‚¬ëŒì´ ì‘ì„±í•œ â€˜ì§€ì‹œâ†”ì¢‹ì€ ë‹µë³€â€™ ì˜ˆì‹œë§Œ ë°˜ë³µ í•™ìŠµí•´ ì§€ì‹œ ë”°ë¥´ê¸° ëŠ¥ë ¥ì„ ë¶™ì¸ë‹¤(SFT).  \n",
      "3. ì‚¬ëŒì´ ì„ í˜¸í•˜ëŠ” ìˆœìœ„ ë°ì´í„°ë¡œ ë³´ìƒ ëª¨ë¸ì„ ë§Œë“¤ê³ , ê°•í™”í•™ìŠµ(PPO)ìœ¼ë¡œ ì‘ë‹µ í’ˆì§ˆì„ ìµœì í™”í•œë‹¤(RLHF).\n",
      "\n",
      "ì´ 3ë‹¨ê³„ë¥¼ í†µí•´ â€œë‹¨ìˆœ í™•ë¥  ê¸°ê³„â€ì˜€ë˜ ëª¨ë¸ì€ â€˜ëŒ€í™”í˜• AIâ€™ë¡œ ë³€ì‹ í•˜ë©°, ì§ˆë¬¸ì— ë‹µí•˜ê³ , ìœ¤ë¦¬ì  í•œê³„ë¥¼ ì§€í‚¤ê³ , ì‚¬ëŒì´ ì„ í˜¸í•˜ëŠ” ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 2-íŠœí”Œ í˜•íƒœì˜ ë©”ì‹œì§€ ëª©ë¡ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± (type, content)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    # role, message\n",
    "    (\"system\", \"This system is an expert in answering questions about {topic}. Please provide clear and detailed explanations.\"),\n",
    "    (\"human\", \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", model_name=\"ChatGPT\")\n",
    "print(messages)\n",
    "\n",
    "# ìƒì„±í•œ ë©”ì‹œì§€ë¥¼ ë°”ë¡œ ì£¼ì…í•˜ì—¬ í˜¸ì¶œí•˜ê¸°\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ChatGPTëŠ” â€œë§Œë“¤ì–´ì§„ ë’¤ì—” ë‹¨ìˆœí•œ ìë™ì™„ì„±ê¸°â€ì§€ë§Œ, ê·¸ ìë™ì™„ì„± ëŠ¥ë ¥ì„ ì–»ê¸°ê¹Œì§€ 3ë‹¨ê³„ì˜ í•™ìŠµ ì ˆì°¨ë¥¼ ê±°ì¹©ë‹ˆë‹¤. ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°, ëª©í‘œ, ìµœì í™” ê¸°ë²•ì´ ë‹¤ë¥´ë©°, ì´ë¥¼ ì´í•´í•˜ë©´ â€œì™œ ChatGPTê°€ ëŒ€í™”ë¥¼ í•  ìˆ˜ ìˆëŠ”ì§€â€ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” 2024ë…„ 4ì›” ê¸°ì¤€ìœ¼ë¡œ ê³µê°œëœ ì •ë³´ë§Œì„ ì •ë¦¬í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n",
      "\n",
      "----------------------------------------\n",
      "1. ë‹¨ê³„ 0: ì‚¬ì „í•™ìŠµ(Pre-training)  \n",
      "   ëª©í‘œ â†’ â€œì„¸ìƒì— ì¡´ì¬í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì••ì¶•â€  \n",
      "   ë°ì´í„° â†’ ê³µê°œ ì›¹, ë„ì„œ, ì½”ë“œ ë“± ìˆ˜ì²œ ì–´íœ˜ í† í°(ì•½ 10 TB ê¸‰)  \n",
      "   ë°©ë²• â†’ â€œë‹¤ìŒ í† í° ì˜ˆì¸¡â€ì„ ìˆ˜ì‹í™”í•œ ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •(Maximum Likelihood)  \n",
      "   ê²°ê³¼ë¬¼ â†’ GPT-ë² ì´ìŠ¤(base) ëª¨ë¸  \n",
      "   í•µì‹¬ í¬ì¸íŠ¸  \n",
      "   - Transformerì˜ ë””ì½”ë”ë§Œ ìŒ“ì€ â€˜ì–¸ì–´ ëª¨ë¸â€™ì´ë©°, í”„ë¡¬í”„íŠ¸ê°€ ì£¼ì–´ì§€ë©´ í™•ë¥ ì ìœ¼ë¡œ í† í°ì„ ì´ì–´ ì”€.  \n",
      "   - ì§€ì‹ì´ ë“¤ì–´ ìˆì§€ë§Œ â€œì‚¬ëŒì´ ì›í•˜ëŠ” ë°©ì‹â€ìœ¼ë¡œ ë‹µì„ í•˜ì§€ëŠ” ì•ŠìŒ(í™€ë£¨ì‹œë„¤ì´ì…˜, í¸í–¥, ì•ˆì „ì„± ë¯¸í™•ë³´).  \n",
      "   - ì´ ì‹œì ì—” â€œëŒ€í™”â€ ë°ì´í„°ëŠ” ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ, ì‚¬ìš©ì ì§ˆë¬¸ì— â€œì™œ ê·¸ëŸ°ì§€ ì„¤ëª…í•´ì¤˜â€ë¼ê³  í•´ë„ ì§ˆë¬¸ ìì²´ë¥¼ ê³„ì† ì´ì–´ ì“°ê¸°ë§Œ í•¨.\n",
      "\n",
      "----------------------------------------\n",
      "2. ë‹¨ê³„ 1: ì§€ë„ íŒŒì¸íŠœë‹(Supervised Fine-Tuning, SFT)  \n",
      "   ëª©í‘œ â†’ â€œëŒ€í™” í˜•ì‹ì„ ìµíˆê³ , ì§€ì‹œì‚¬í•­ì„ ë”°ë¥´ëŠ” ìŠµê´€ ê¸°ë¥´ê¸°â€  \n",
      "   ë°ì´í„° â†’ ëŒ€í™”Â·ì§€ì‹œÂ·ì‘ë‹µ ìŒ 10~100k ìˆ˜ì¤€(í’ˆì§ˆ ë†’ì€ â€˜êµì‚¬ ë°ì´í„°â€™)  \n",
      "   ë°©ë²• â†’ ë™ì¼í•œ ìµœëŒ€ê°€ëŠ¥ë„ í•™ìŠµ, ë‹¤ë§Œ ë ˆì´ë¸”=â€˜ì˜¬ë°”ë¥¸ ì‘ë‹µâ€™ì¸ ì§€ì‹œì‘ë‹µ ìŒ  \n",
      "   ê²°ê³¼ë¬¼ â†’ SFT ëª¨ë¸(â€˜GPT-ì§€ì‹œâ€™ë¼ê³ ë„ ë¶ˆë¦¼)  \n",
      "   í•µì‹¬ í¬ì¸íŠ¸  \n",
      "   - â€œí”„ë¡¬í”„íŠ¸: A, ì‘ë‹µ: Bâ€ í˜•íƒœë¥¼ ë°˜ë³µ í•™ìŠµí•´, ëª¨ë¸ì´ â€œì§€ì‹œë¥¼ ëë‚´ê³  ì‘ë‹µì„ ì‹œì‘â€í•˜ëŠ” íŒ¨í„´ì„ ìµí˜.  \n",
      "   - ì—¬ì „íˆ â€œì–¸ì œë“  ì˜ëª»ëœ ë‹µì„ ìƒì„±â€í•  ìˆ˜ ìˆìŒ. ì•ˆì „ì„±Â·ì§„ì‹¤ì„± í™•ë³´ëŠ” ëª» í•¨.\n",
      "\n",
      "----------------------------------------\n",
      "3. ë‹¨ê³„ 2: ê°•í™”í•™ìŠµ(RLHF, Reinforcement Learning from Human Feedback)  \n",
      "   ëª©í‘œ â†’ â€œì¸ê°„ì´ â€˜ë” ë‚«ë‹¤â€™ê³  ëŠë¼ëŠ” ë‹µë³€ì„ í™•ë¥ ì ìœ¼ë¡œ ì¦ê°€â€  \n",
      "   êµ¬ì„±ìš”ì†Œ  \n",
      "   a) ë³´ìƒ ëª¨ë¸(Reward Model, RM)  \n",
      "      - SFT ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ ì—¬ëŸ¬ ê°œë¥¼ ì‚¬ëŒì´ ìˆœìœ„ ë§¤ê¹€(â‰ˆ70k ê°œ)  \n",
      "      - RMì€ â€œì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸-ë‹µë³€ ìŒì— ëŒ€í•œ ì ìˆ˜â€ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ íšŒê·€í•™ìŠµ  \n",
      "   b) ì •ì±…(policy) = RLHF ëŒ€ìƒ ëª¨ë¸  \n",
      "      - PPO(Proximal Policy Optimization)ë¡œ RM ì ìˆ˜â†‘, KL-ì •ê·œë¦¬ì¦˜(ê¸°ì¡´ SFT ëª¨ë¸ê³¼ ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šë„ë¡)â†“  \n",
      "   ê²°ê³¼ë¬¼ â†’ ìµœì¢… ChatGPT ëª¨ë¸  \n",
      "   í•µì‹¬ í¬\n"
     ]
    }
   ],
   "source": [
    "# ì²´ì¸ì„ ìƒì„±í•˜ì—¬ í˜¸ì¶œí•˜ê¸°\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\":\"AI\", \"model_name\":\"ChatGPT\"})\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ChatPromptTemplate\n",
    "* SystemMessagePromptTemplateì™€ HumanMessagePromptTemplate í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "* ê°ì²´ ì§€í–¥ì  ì ‘ê·¼ - Message ê°ì²´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìƒì„± ê°€ëŠ¥\n",
    "* ì—¬ëŸ¬ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„ íƒ\n",
    "\n",
    "```python\n",
    "if user_is_beginner:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª…: {topic}\")\n",
    "else:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ìƒì„¸ ë¶„ì„: {topic}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ChatMessagePromptTemplate í™œìš©\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê°œë³„ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI expert in {topic}. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateë¡œ ë©”ì‹œì§€ë“¤ì„ ë¬¶ê¸°\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒì„±\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"What is deep learning?\")\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatMessagePromptTemplateëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë©”ì‹œì§€(ì‹œìŠ¤í…œ, ì¸ê°„, AI)ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "* SystemMessagePromptTemplate: ì´ í…œí”Œë¦¿ì€ AI ëª¨ë¸ì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ê±°ë‚˜ ì „ë°˜ì ì¸ ê·œì¹™ì„ ì„¤ì •í•˜ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” \"ë²ˆì—­ì„ ë„ì™€ì£¼ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸\"ë¼ëŠ” ì—­í• ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "* HumanMessagePromptTemplate: ì´ í…œí”Œë¦¿ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ë‹´ëŠ” ì¸ê°„ ë©”ì‹œì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œì—ì„œëŠ” ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.\n",
    "* ChatPromptTemplate.from_messages: ì´ í´ë˜ìŠ¤ ë©”ì„œë“œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì¸ê°„ ë©”ì‹œì§€ ë“± ì—¬ëŸ¬ ì¢…ë¥˜ì˜ MessagePromptTemplate ê°ì²´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ í•˜ë‚˜ì˜ ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.\n",
    "* format_messages: ì´ ë©”ì„œë“œëŠ” ì •ì˜ëœ í…œí”Œë¦¿ì— ì‹¤ì œ ê°’ì„ ì±„ì›Œ ë„£ì–´ [SystemMessage, HumanMessage] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ë¦¬ìŠ¤íŠ¸ëŠ” ì±„íŒ… ëª¨ë¸(Chat Model) ì— ë°”ë¡œ ì „ë‹¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant that translates English to Korean.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n",
      "ë‚˜ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ì‚¬ë‘í•´.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# 1. SystemMessagePromptTemplateì™€ HumanMessagePromptTemplate ìƒì„±\n",
    "# SystemMessagePromptTemplateëŠ” ëª¨ë¸ì˜ í˜ë¥´ì†Œë‚˜ ë˜ëŠ” ê¸°ë³¸ ì§€ì¹¨ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# HumanMessagePromptTemplateëŠ” ì‚¬ìš©ìë¡œë¶€í„° ë°›ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "human_template = \"{text_to_translate}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 2. ChatPromptTemplate ìƒì„±\n",
    "# ìœ„ì—ì„œ ë§Œë“  ë‘ í…œí”Œë¦¿ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ ChatPromptTemplateì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 3. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "# chat_prompt_template.format_messages()ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# ì´ í•¨ìˆ˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥ ë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.\n",
    "formatted_prompt = chat_prompt_template.format_messages(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"Korean\",\n",
    "    text_to_translate=\"I love programming.\"\n",
    ")\n",
    "\n",
    "# 4. ê²°ê³¼ ì¶œë ¥\n",
    "print(formatted_prompt)\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) FewShotPromptTemplate\n",
    "* FewShotPromptTemplateì€ ëª¨ë¸ì´ íŠ¹ì • í˜•ì‹ì„ ë”°ë¥´ê²Œ í•˜ê±°ë‚˜, ì¼ê´€ëœ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "* ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”í•˜ê±°ë‚˜, AIê°€ ì˜¤ë‹µì„ ì¤„ì´ê³  ë” ì‹ ë¢°í•  ë§Œí•œ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•´ì•¼ í•  ë•Œ íš¨ê³¼ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1) PromptTemplateì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "íƒœì–‘ê³„ í–‰ì„±ì„ â€œë°”ìœ„â€ì™€ â€œê°€ìŠ¤â€ ë‘ ë©ì–´ë¦¬ë¡œ ë‚˜ëˆ  í•œ ì¤„ì”© ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìˆ˜ì„± Â· ê¸ˆì„± Â· ì§€êµ¬ Â· í™”ì„±  \n",
      "ë•…ë©ì–´ë¦¬(ì•”í–‰ì„±)ì´ê³ , ê¼¬ë¦¬ë³„ì²˜ëŸ¼ ì–‡ì€ ëŒ€ê¸°ë§Œ ì”Œì›Œì ¸ ìˆë‹¤.\n",
      "\n",
      "ëª©ì„± Â· í† ì„± Â· ì²œì™•ì„± Â· í•´ì™•ì„±  \n",
      "ê±°ëŒ€ ê°€ìŠ¤ë©ì–´ë¦¬(ëª©í–‰ì„±)ë¡œ, ê³ ë¦¬Â·ìœ„ì„±Â·ê·¹ê´‘ ë“± ì¥ì‹ êµ¬ê°€ í™”ë ¤í•˜ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplateì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# chain ì‹¤í–‰\n",
    "result = llm.invoke(\"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-2) FewShotChatMessagePromptTemplate ì‚¬ìš©í•˜ëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.', 'output': '### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\\n1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\\n2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\\n3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.'}, {'input': 'ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.', 'output': '### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\\n- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\\n- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\\n- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\\n- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000269CB831940>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000269CB7DAE70>, root_client=<openai.OpenAI object at 0x00000269CB85A930>, root_async_client=<openai.AsyncOpenAI object at 0x00000269CB85A960>, model_name='openai/gpt-oss-120b', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "## íƒœì–‘ê³„ì˜ í–‰ì„± 8ê°œ (ê°„ë‹¨ ì •ë¦¬)\n",
      "\n",
      "| ìˆœì„œ(íƒœì–‘ì—ì„œ ë©€ì–´ì§€ëŠ” ìˆœì„œ) | í–‰ì„± | ì¢…ë¥˜ | íŠ¹ì§• (ì‰½ê²Œ ë§í•˜ë©´) |\n",
      "|---|---|---|---|\n",
      "| 1 | **ìˆ˜ì„±** | ì•”ì„ í–‰ì„± | íƒœì–‘ì— ê°€ì¥ ê°€ê¹Œì›Œì„œ ë§¤ìš° ëœ¨ê²ê³ , ë°¤ì—ëŠ” ì•„ì£¼ ì¶”ì›Œìš”. |\n",
      "| 2 | **ê¸ˆì„±** | ì•”ì„ í–‰ì„± | â€œì§€êµ¬ì˜ ìŒë‘¥ì´â€ë¼ê³  ë¶ˆë¦¬ì§€ë§Œ, ëŒ€ê¸°ê°€ ë‘ê»ê³  ì˜¨ë„ê°€ 400â€¯Â°C ì´ìƒì´ë¼ ì•„ì£¼ ëœ¨ê±°ì›Œìš”. |\n",
      "| 3 | **ì§€êµ¬** | ì•”ì„ í–‰ì„± | ë¬¼ê³¼ ìƒëª…ì´ ìˆëŠ” ìœ ì¼í•œ í–‰ì„±! ëŒ€ê¸°ì˜ 78%ëŠ” ì§ˆì†Œ, 21%ëŠ” ì‚°ì†Œì˜ˆìš”. |\n",
      "| 4 | **í™”ì„±** | ì•”ì„ í–‰ì„± | ë¶‰ì€ ìƒ‰ì„ ë ê³ , ì˜›ë‚ ì— ë¬¼ì´ ìˆì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆì–´ìš”. í˜„ì¬ëŠ” ì–‡ì€ ì´ì‚°í™”íƒ„ì†Œ ëŒ€ê¸°ë§Œ ìˆì–´ìš”. |\n",
      "| 5 | **ëª©ì„±** | ê°€ìŠ¤ í–‰ì„± | ê°€ì¥ í° í–‰ì„±. ë¹¨ê°„ ìƒ‰ ì (ëŒ€ì ì )ì€ ê±°ëŒ€í•œ í­í’ì´ì—ìš”. 79ê°œì˜ ìœ„ì„±ì„ ê°€ì§€ê³  ìˆë‹µë‹ˆë‹¤. |\n",
      "| 6 | **í† ì„±** | ê°€ìŠ¤ í–‰ì„± | ì•„ë¦„ë‹¤ìš´ ê³ ë¦¬(ì–¼ìŒê³¼ ë°”ìœ„ ì¡°ê°)ë¡œ ìœ ëª…í•´ìš”. ê³ ë¦¬ ì•ˆìª½ì— ì‘ì€ ìœ„ì„±ë„ ë§ì•„ìš”. |\n",
      "| 7 | **ì²œì™•ì„±** | ì–¼ìŒÂ·ê°€ìŠ¤ í–‰ì„± | ìì „ì¶•ì´ ê±°ì˜ ì˜†ìœ¼ë¡œ ê¸°ìš¸ì–´ì ¸ ìˆì–´, â€˜ì˜†ìœ¼ë¡œ ëˆ„ìš´ í–‰ì„±â€™ì´ë¼ê³  ë¶ˆë ¤ìš”. í‘¸ë¥¸ ìƒ‰ì€ ë©”íƒ„ ê°€ìŠ¤ ë•Œë¬¸ì´ì—ìš”. |\n",
      "| 8 | **í•´ì™•ì„±** | ì–¼ìŒÂ·ê°€ìŠ¤ í–‰ì„± | ê°€ì¥ ë°”ê¹¥ìª½ í–‰ì„±. ë°”ëŒì´ ì•„ì£¼ ë¹ ë¥´ê²Œ ë¶ˆê³ , í‘¸ë¥¸ ìƒ‰ì€ ë©”íƒ„ ë•Œë¬¸ì— ë‚˜íƒ€ë‚˜ìš”. |\n",
      "\n",
      "### ê¸°ì–µí•˜ë©´ ì¢‹ì€ í¬ì¸íŠ¸\n",
      "- **ë‚´í–‰ì„±(ì•”ì„ í–‰ì„±)**: ìˆ˜ì„±, ê¸ˆì„±, ì§€êµ¬, í™”ì„± â†’ í¬ê¸°ê°€ ì‘ê³  í‘œë©´ì´ ë°”ìœ„ë¡œ ì´ë£¨ì–´ì§.  \n",
      "- **ì™¸í–‰ì„±(ê°€ìŠ¤Â·ì–¼ìŒ í–‰ì„±)**: ëª©ì„±, í† ì„±, ì²œì™•ì„±, í•´ì™•ì„± â†’ í¬ê¸°ê°€ í¬ê³  ëŒ€ë¶€ë¶„ì´ ê°€ìŠ¤ì™€ ì–¼ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§.  \n",
      "- **íƒœì–‘ê³„ëŠ” 8ê°œì˜ í–‰ì„±** (2006ë…„ êµ­ì œì²œë¬¸ì—°ë§¹ì´ ëª…ì™•ì„±ì„ â€˜ì™œì†Œ í–‰ì„±â€™ìœ¼ë¡œ ì¬ë¶„ë¥˜í•˜ë©´ì„œ 8ê°œê°€ ëì–´ìš”).  \n",
      "\n",
      "ì´ë ‡ê²Œ íƒœì–‘ì„ ì¤‘ì‹¬ìœ¼ë¡œ 8ê°œì˜ í–‰ì„±ì´ ì°¨ë¡€ëŒ€ë¡œ ë‘¥ê¸€ê²Œ ëŒê³  ìˆë‹µë‹ˆë‹¤! ğŸš€ğŸŒ\n"
     ]
    }
   ],
   "source": [
    "# FewShotChatMessagePromptTemplate ì‚¬ìš©í•˜ëŠ” ê²½ìš°\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\n",
    "1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\n",
    "- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\n",
    "- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate ì ìš©\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "chain = final_prompt | llm\n",
    "print(chain)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-1) PartialPrompt \n",
    "* í”„ë¡¬í”„íŠ¸ë¥¼ ë” ë™ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, AI ì‘ë‹µì„ ë” ì¼ê´€ì„± ìˆê²Œ ì¡°ì • ê°€ëŠ¥í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " í”„ë¡¬í”„íŠ¸: ê°€ì„ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ íƒœí’ ë°œìƒì´ ë§ë‚˜ìš”? ê°€ì„ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì§€êµ¬ê³¼í•™ í˜„ìƒì„ 3ê°œ ì•Œë ¤ì£¼ì„¸ìš”\n",
      " ëª¨ë¸ ì‘ë‹µ: ê°€ì„ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒìœ¼ë¡œ **íƒœí’(ì—´ëŒ€ì €ê¸°ì••)**ì„ ê¼½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œêµ­ì„ ë¹„ë¡¯í•œ ë™ì•„ì‹œì•„ ì§€ì—­ì—ì„œëŠ” 8ì›” ë§ë¶€í„° 10ì›” ì´ˆê¹Œì§€ íƒœí’ì´ ê°€ì¥ ë§ì´ ë°œìƒí•˜ê³ , ê°€ì„ì´ ë ìˆ˜ë¡ ë‚¨ìª½ì—ì„œ ë¶ìª½ìœ¼ë¡œ ì´ë™í•˜ë©´ì„œ í•œë°˜ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë‹¤ë§Œ â€œê°€ì„ì—ë§Œâ€ ì¼ì–´ë‚˜ëŠ” í˜„ìƒì€ ì•„ë‹ˆë©°, ë‹¤ë¥¸ ê³„ì ˆì—ë„ ë‚˜íƒ€ë‚  ìˆ˜ ìˆëŠ” í˜„ìƒê³¼ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ì•„ë˜ëŠ” **ê°€ì„ì— íŠ¹íˆ í™œë°œíˆ ë‚˜íƒ€ë‚˜ëŠ” ì£¼ìš” ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€**ì…ë‹ˆë‹¤.\n",
      "\n",
      "| í˜„ìƒ | ì£¼ìš” íŠ¹ì§• ë° ì›ì¸ | ê°€ì„ì— ë‘ë“œëŸ¬ì§€ëŠ” ì´ìœ  |\n",
      "|------|-------------------|------------------------|\n",
      "| **1. íƒœí’Â·ì—´ëŒ€ì €ê¸°ì••** | ë°”ë‹¤ ìœ„ì—ì„œ í˜•ì„±ëœ ì €ê¸°ì••ì´ ê°•í•œ ë°”ëŒÂ·í­ìš°Â·í•´ì¼ì„ ë™ë°˜í•¨. ë‚¨ì„œìª½ì˜ ì˜¨ë‚œÂ·ë‹¤ìŠµí•œ í•´ìˆ˜ì™€ ëŒ€ê¸° ë¶ˆì•ˆì •ì´ í•„ìš”. | ê°€ì„ì´ ë˜ë©´ ì„œìª½ íƒœí‰ì–‘ì˜ í•´ìˆ˜ë©´ ì˜¨ë„ê°€ ì•„ì§ ë†’ì•„ íƒœí’ì´ ê°•í•˜ê²Œ ë°œë‹¬í•˜ê³ , ì œíŠ¸ê¸°ë¥˜ê°€ ë‚¨ìª½ìœ¼ë¡œ ë‚´ë ¤ì™€ íƒœí’ ê²½ë¡œë¥¼ í•œë°˜ë„Â·ë™ì•„ì‹œì•„ë¡œ ëŒì–´ë‹¹ê¹ë‹ˆë‹¤. |\n",
      "| **2. ê°€ì„ì²  ëŒ€ê¸° ì—­ì „Â·ì—°ë¬´(ì•ˆê°œ)Â·ì—°ë¬´ í˜„ìƒ** | ì§€í‘œë©´ì´ ë¹ ë¥´ê²Œ ëƒ‰ê°ë˜ë©´ì„œ ì°¨ê°€ìš´ ê³µê¸°ê°€ ì§€í‘œë©´ì— ë¨¸ë¬¼ê³ , ìœ„ìª½ì— ë”°ëœ»í•œ ê³µì¸µì´ í˜•ì„±ë¼ ëŒ€ê¸° ì—­ì „ì´ ë°œìƒ. ìˆ˜ì¦ê¸°ê°€ ì‘ê²°í•´ ì•ˆê°œÂ·ì—°ë¬´ê°€ ìì£¼ ë‚˜íƒ€ë‚¨. | ê°€ì„ì€ ì¼êµì°¨ê°€ í¬ê²Œ ëŠ˜ê³ , ë°¤ì— ê¸‰ê²©íˆ ëƒ‰ê°ë˜ëŠ” ê²½ìš°ê°€ ë§ì•„ ëŒ€ê¸° ì—­ì „ì´ ì‰½ê²Œ í˜•ì„±ë©ë‹ˆë‹¤. íŠ¹íˆ ê°•ì›ë„Â·ë‚´ë¥™ ì§€ë°©ì—ì„œ ì‚°ì•… ì•ˆê°œ, í‰ì•¼ì—ì„œëŠ” ì—°ë¬´ê°€ ìì£¼ ë³´ì…ë‹ˆë‹¤. |\n",
      "| **3. ê°€ì„ ê°•ìˆ˜Â·ì „ì„  í™œë™ ê°•í™”** | í•œë°˜ë„ëŠ” ëŒ€ë¥™ì„± ê³ ê¸°ì••(ì‹œë² ë¦¬ì•„ ê³ ê¸°ì••)ê³¼ ë‚¨ìª½ì˜ ë‚¨ë™ì•„ì‹œì•„ ì €ê¸°ì••ì´ êµì°¨í•˜ë©´ì„œ ì „ì„ ì´ í™œë°œíˆ ì›€ì§ì„. ì´ë•Œ ê°•ìˆ˜ì™€ ê¸‰ê²©í•œ ê¸°ì˜¨ ë³€í™”ê°€ ì¼ì–´ë‚¨. | ê°€ì„ì— ì‹œë² ë¦¬ì•„ ê³ ê¸°ì••ì´ ë‚¨í•˜í•˜ë©´ì„œ ì°¨ê°€ìš´ ë¶í’ì´ ë‚¨ìª½ì˜ ì˜¨ë‚œÂ·ë‹¤ìŠµí•œ ê³µê¸°ì™€ ë§Œë‚˜ ì „ì„ ì„ í˜•ì„±í•©ë‹ˆë‹¤. ì „ì„ ì´ í†µê³¼í•  ë•ŒëŠ” ë¹„Â·ëˆˆÂ·ìš°ë°•ì´ ë™ë°˜ë˜ëŠ” ê°•ìˆ˜ê°€ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤. |\n",
      "\n",
      "### ê°„ë‹¨íˆ ì •ë¦¬í•˜ë©´\n",
      "\n",
      "1. **íƒœí’** â€“ ê°€ì„ì— ê°€ì¥ í™œë°œíˆ ë°œìƒí•˜ê³ , í•œë°˜ë„ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ëŒ€í‘œì ì¸ í˜„ìƒ.  \n",
      "2. **ëŒ€ê¸° ì—­ì „Â·ì•ˆê°œÂ·ì—°ë¬´** â€“ ê¸‰ê²©í•œ ì¼êµì°¨ì™€ ì°¨ê°€ìš´ ì§€í‘œë©´ ë•Œë¬¸ì— ê°€ì„ì— í”íˆ ë‚˜íƒ€ë‚¨.  \n",
      "3. **ê°€ì„ ê°•ìˆ˜Â·ì „ì„  í™œë™** â€“ ì‹œë² ë¦¬ì•„ ê³ ê¸°ì••ê³¼ ë‚¨ë™ì•„ì‹œì•„ ì €ê¸°ì••ì˜ êµì°¨ë¡œ ì¸í•´ ê°•ìˆ˜ì™€ ê¸‰ê²©í•œ ê¸°ì˜¨ ë³€í™”ê°€ ë¹ˆë²ˆ.\n",
      "\n",
      "ì´ ì„¸ ê°€ì§€ í˜„ìƒì€ ëª¨ë‘ **ê°€ì„ì— íŠ¹íˆ ë‘ë“œëŸ¬ì§€ì§€ë§Œ**, ë‹¤ë¥¸ ê³„ì ˆì—ë„ ì–´ëŠ ì •ë„ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤. í•„ìš”ì— ë”°ë¼ ë” ìì„¸í•œ ë©”ì»¤ë‹ˆì¦˜ì´ë‚˜ ì§€ì—­ë³„ íŠ¹ì„±ì„ ì•Œë ¤ë“œë¦´ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì–¸ì œë“  ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ ì§ˆë¬¸ ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë¶€ë¶„ ë³€ìˆ˜ ì ìš©)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ {phenomenon}ì´ ë§ë‚˜ìš”? {season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì§€êµ¬ê³¼í•™ í˜„ìƒì„ 3ê°œ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "    input_variables=[\"phenomenon\"],  # ì‚¬ìš©ì ì…ë ¥ í•„ìš”\n",
    "    partial_variables={\"season\": get_current_season()}  # ë™ì ìœ¼ë¡œ ê³„ì ˆ ê°’ í• ë‹¹\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# íŠ¹ì • ê³„ì ˆì˜ í˜„ìƒ ì§ˆì˜\n",
    "query = prompt.format(phenomenon=\"íƒœí’ ë°œìƒ\")\n",
    "result = llm.invoke(query)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\" í”„ë¡¬í”„íŠ¸: {query}\")\n",
    "print(f\" ëª¨ë¸ ì‘ë‹µ: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ê³„ì ˆ: ê°€ì„\n",
      "\n",
      " ê°€ì„ì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\n",
      "1.  **ì„±ìš´**: ê°€ì„ ë°¤ì— ë°ì€ ë³„ìë¦¬ì¸ í˜ê°€ìˆ˜ìŠ¤ìë¦¬ ê·¼ì²˜ì—ì„œ ê´€ì°°í•  ìˆ˜ ìˆëŠ” ì„±ìš´ì€ ê°€ì„ì˜ ëŒ€í‘œì ì¸ ì²œë¬¸ í˜„ìƒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ì‹œê¸°ì— ê³µê¸°ê°€ ë§‘ê³  ì¶”ìš´ ë‚ ì”¨ë¡œ ì¸í•´ ë°¤í•˜ëŠ˜ì—ì„œ ë” ë§ì€ ë³„ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì‹œê¸°ì—ëŠ” ì„±ìš´ì´ ì„ ëª…í•˜ê²Œ ë‚˜íƒ€ë‚  ë•Œê°€ ë§ìŠµë‹ˆë‹¤.\n",
      "2.  **ìˆ˜í™•ì›”**: ì§€êµ¬ê°€ íƒœì–‘ì„ ì¤‘ì‹¬ìœ¼ë¡œ íšŒì „í•˜ë©´ì„œ ê°€ì„ì—ëŠ” ë°¤ê³¼ ë‚®ì˜ ê¸¸ì´ê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ë‚®ì´ ì§§ì•„ì§€ê³  ë°¤ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ê¸°ì˜¨ì´ ë”ìš± ë‚®ì•„ì§€ê³  ì¶”ìš´ ë‚ ì”¨ê°€ ì‹œì‘ë©ë‹ˆë‹¤. ì´ ì‹œê¸°ì— ë†ì‘ë¬¼ì´ ìµê³  ìˆ˜í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "3.  **ê°€ì„ ë‹¨í’**: ê°€ì„ì—ëŠ” ì¼êµì°¨ë¡œ ì¸í•´ ë°¤ê³¼ ë‚®ì˜ ê¸°ì˜¨ì´ í¬ê²Œ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ì˜¨ ë³€í™”ë¡œ ì¸í•´ ë‚˜ë¬´ì˜ ìì´ ìƒ‰ì´ ë³€í•˜ê³  ì˜ˆìœ ë‹¨í’ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ë¶‰ì€ìƒ‰, ë…¸ë€ìƒ‰, ì£¼í™©ìƒ‰ ë“± ë‹¤ì–‘í•œ ìƒ‰ê¹”ì˜ ë‹¨í’ì´ ê³„ì ˆì˜ ë³€í™”ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# Step 1: í˜„ì¬ ê³„ì ˆ ê²°ì •\n",
    "season = get_current_season(\"north\")  # ê³„ì ˆ ê°’ ì–»ê¸°\n",
    "print(f\"í˜„ì¬ ê³„ì ˆ: {season}\")\n",
    "\n",
    "# Step 2: í•´ë‹¹ ê³„ì ˆì˜ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. \"\n",
    "    \"ê° í˜„ìƒì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# ì²´ì¸ 2: ìì—° í˜„ìƒ ì¶”ì²œ (ì…ë ¥: ê³„ì ˆ â†’ ì¶œë ¥: ìì—° í˜„ìƒ ëª©ë¡)\n",
    "chain2 = (\n",
    "    {\"season\": lambda x : season}  # chain1ì˜ ì¶œë ¥ì„ season ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰: í˜„ì¬ ê³„ì ˆì— ë”°ë¥¸ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "response = chain2.invoke({})\n",
    "print(f\"\\n {season}ì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'season_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     17\u001b[39m chain2 = (\n\u001b[32m     18\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x : season_name}  \u001b[38;5;66;03m# chain1ì˜ ì¶œë ¥ì„ season ë³€ìˆ˜ë¡œ ì „ë‹¬\u001b[39;00m\n\u001b[32m     19\u001b[39m     | prompt2\n\u001b[32m     20\u001b[39m     | llm\n\u001b[32m     21\u001b[39m     | StrOutputParser()\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ì‹¤í–‰: í˜„ì¬ ê³„ì ˆì— ë”°ë¥¸ ìì—° í˜„ìƒ ì¶”ì²œ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m response = \u001b[43mchain2\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3243\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3241\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3243\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3244\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3245\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4000\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3995\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3996\u001b[39m         futures = [\n\u001b[32m   3997\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3998\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3999\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m4000\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   4001\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   4002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3984\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3978\u001b[39m child_config = patch_config(\n\u001b[32m   3979\u001b[39m     config,\n\u001b[32m   3980\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3981\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3982\u001b[39m )\n\u001b[32m   3983\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3984\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3986\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5024\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this ``Runnable`` synchronously.\u001b[39;00m\n\u001b[32m   5010\u001b[39m \n\u001b[32m   5011\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5021\u001b[39m \n\u001b[32m   5022\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m5024\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5025\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5026\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5028\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5030\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5031\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2089\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2085\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2086\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2087\u001b[39m         output = cast(\n\u001b[32m   2088\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2089\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2091\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2093\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2094\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2096\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2097\u001b[39m         )\n\u001b[32m   2098\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2099\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4881\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4879\u001b[39m                 output = chunk\n\u001b[32m   4880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4881\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      9\u001b[39m llm = ChatOpenAI(\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m#api_key=OPENAI_API_KEY,\u001b[39;00m\n\u001b[32m     11\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.groq.com/openai/v1\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Groq API ì—”ë“œí¬ì¸íŠ¸\u001b[39;00m\n\u001b[32m     12\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/llama-4-scout-17b-16e-instruct\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\u001b[39;00m\n\u001b[32m     13\u001b[39m     temperature=\u001b[32m0.0\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# ì²´ì¸ 2: ìì—° í˜„ìƒ ì¶”ì²œ (ì…ë ¥: ê³„ì ˆ â†’ ì¶œë ¥: ìì—° í˜„ìƒ ëª©ë¡)\u001b[39;00m\n\u001b[32m     17\u001b[39m chain2 = (\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mseason_name\u001b[49m}  \u001b[38;5;66;03m# chain1ì˜ ì¶œë ¥ì„ season ë³€ìˆ˜ë¡œ ì „ë‹¬\u001b[39;00m\n\u001b[32m     19\u001b[39m     | prompt2\n\u001b[32m     20\u001b[39m     | llm\n\u001b[32m     21\u001b[39m     | StrOutputParser()\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ì‹¤í–‰: í˜„ì¬ ê³„ì ˆì— ë”°ë¥¸ ìì—° í˜„ìƒ ì¶”ì²œ\u001b[39;00m\n\u001b[32m     25\u001b[39m response = chain2.invoke({})\n",
      "\u001b[31mNameError\u001b[39m: name 'season_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2: í•´ë‹¹ ê³„ì ˆì˜ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. \"\n",
    "    \"ê° í˜„ìƒì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# ì²´ì¸ 2: ìì—° í˜„ìƒ ì¶”ì²œ (ì…ë ¥: ê³„ì ˆ â†’ ì¶œë ¥: ìì—° í˜„ìƒ ëª©ë¡)\n",
    "chain2 = (\n",
    "    {\"season\": lambda x : season_name}  # chain1ì˜ ì¶œë ¥ì„ season ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰: í˜„ì¬ ê³„ì ˆì— ë”°ë¥¸ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "response = chain2.invoke({})\n",
    "print(f\"\\n {season_name}ì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2) PartialPrompt \n",
    "* API í˜¸ì¶œ ë°ì´í„°, ì‹œê°„ ì •ë³´, ì‚¬ìš©ì ì •ë³´ ë“±ì„ ë°˜ì˜í•  ë•Œ ë§¤ìš° ìœ ìš©í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " í”„ë¡¬í”„íŠ¸: í˜„ì¬ 1ë‹¬ëŸ¬ = 1377.98ì› ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\n",
      " ëª¨ë¸ ì‘ë‹µ: ## **2024ë…„ 4ì›” 5ì¼ í™˜ìœ¨ ë¶„ì„**\n",
      "\n",
      "### **1. í˜„ì¬ í™˜ìœ¨: 1ë‹¬ëŸ¬ = 1377.98ì›**\n",
      "\n",
      "2024ë…„ 4ì›” 5ì¼, 1ë‹¬ëŸ¬ì˜ ê°€ì¹˜ëŠ” 1377.98ì›ìœ¼ë¡œ í‰ê°€ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì›í™” ê°€ì¹˜ì˜ ì•½ì„¸ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì•„ë˜ëŠ” ë‹¤ì–‘í•œ ì¸¡ë©´ì—ì„œ ì´ í™˜ìœ¨ì— ëŒ€í•œ ë¶„ì„ì…ë‹ˆë‹¤.\n",
      "\n",
      "### **2. ê²½ì œ ì§€í‘œ ë¶„ì„**\n",
      "\n",
      "- **ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©:** ìµœê·¼ ë‹¬ëŸ¬í™” ê°•ì„¸ì˜ ì£¼ëœ ìš”ì¸ ì¤‘ í•˜ë‚˜ëŠ” ë¯¸êµ­ ê²½ì œì˜ ê²¬ì¡°í•œ ì„±ì¥ì„¸ì™€ ì¸í”Œë ˆì´ì…˜ ì••ë ¥ì…ë‹ˆë‹¤. ë¯¸êµ­ì˜ ë†’ì€ ê¸ˆë¦¬ ìˆ˜ì¤€ì´ íˆ¬ììë“¤ì—ê²Œ ë§¤ë ¥ì ì¸ ìˆ˜ìµì„ ì œê³µí•˜ê³  ìˆì–´ ë‹¬ëŸ¬ì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "- **í•œêµ­ ê²½ì œ:** í•œêµ­ ê²½ì œëŠ” ëŒ€ì™¸ ë¬´ì—­ ì˜ì¡´ë„ê°€ ë†’ê¸° ë•Œë¬¸ì— í™˜ìœ¨ ë³€ë™ì— ë¯¼ê°í•©ë‹ˆë‹¤. ìˆ˜ì¶œ ì¤‘ì‹¬ì˜ ê²½ì œ êµ¬ì¡°ë¡œ ì¸í•´ ì•½í•œ ì›í™”ê°€ ìˆ˜ì¶œì— ê¸ì •ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆì§€ë§Œ, ìˆ˜ì… ë¬¼ê°€ì˜ ìƒìŠ¹ìœ¼ë¡œ ì´ì–´ì ¸ êµ­ë‚´ ì¸í”Œë ˆì´ì…˜ì„ ìê·¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### **3. ì •ì±…ì  ìš”ì¸**\n",
      "\n",
      "- **í•œêµ­ì€í–‰:** í•œêµ­ì€í–‰ì€ ìµœê·¼ ê¸ˆë¦¬ ë™ê²°ì„ ê²°ì •í•˜ëŠ” ë“± ê³ ë¬¼ê°€ì™€ ê²½ê¸° ë‘”í™” ìš°ë ¤ ì†ì—ì„œ í†µí™”ì •ì±…ì„ ì‹ ì¤‘í•˜ê²Œ ìš´ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ê¸ˆë¦¬ ì¸ìƒì´ë‚˜ ì¸í•˜ ì—¬ë¶€ëŠ” ê²½ì œ ìƒí™©ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "- **FED(ì—°ì¤€):** ë¯¸êµ­ ì—°ë°©ì¤€ë¹„ì œë„(FED)ëŠ” ì¸í”Œë ˆì´ì…˜ ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ ê¸ˆë¦¬ë¥¼ ì¡°ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. í–¥í›„ ê¸ˆë¦¬ ì •ì±…ì´ ë‹¬ëŸ¬ ê°€ì¹˜ì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "### **4. ì‹œì¥ ì „ë§**\n",
      "\n",
      "- **ë‹¨ê¸° ì „ë§:** ë‹¨ê¸°ì ìœ¼ë¡œëŠ” ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©ê³¼ ì£¼ìš” ê²½ì œêµ­ì˜ í†µí™” ì •ì±…ì— ë”°ë¼ í™˜ìœ¨ì´ ë³€ë™í•  ê²ƒì…ë‹ˆë‹¤. ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ë‚˜ ë¬´ì—­ ê´€ë ¨ ì´ìŠˆë„ í™˜ìœ¨ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "- **ì¥ê¸° ì „ë§:** ì¥ê¸°ì ìœ¼ë¡œëŠ” í•œêµ­ ê²½ì œì˜ í€ë”ë©˜í„¸ê³¼ ê¸€ë¡œë²Œ ê²½ì œì˜ íë¦„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì›í™” ê°€ì¹˜ì˜ ì•ˆì •í™” ì—¬ë¶€ëŠ” ì¸í”Œë ˆì´ì…˜ ê´€ë¦¬, ë¬´ì—­ ìˆ˜ì§€, ì™¸êµ­ì¸ íˆ¬ì ìœ ì¹˜ ë“±ì— ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### **5. ê°œì¸ ë° ê¸°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥**\n",
      "\n",
      "- **ìˆ˜ì¶œì… ê¸°ì—…:** ì›í™” ì•½ì„¸ëŠ” ìˆ˜ì¶œì… ê¸°ì—…ì— í˜¼í•©ëœ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ìˆ˜ì¶œì„ ì´‰ì§„í•  ìˆ˜ ìˆì§€ë§Œ, ìˆ˜ì… ì›ìì¬ ê°€ê²© ìƒìŠ¹ìœ¼ë¡œ ë¹„ìš© ë¶€ë‹´ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "- **ì—¬í–‰ê° ë° í•´ì™¸ íˆ¬ì:** ê°œì¸ì—ê²ŒëŠ” í•´ì™¸ì—¬í–‰ ë¹„ìš© ì¦ê°€, í•´ì™¸ ìì‚° ê°€ì¹˜ ë³€ë™ ë“± ë‹¤ì–‘í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
      "\n",
      "### **6. ê²°ë¡ **\n",
      "\n",
      "í˜„ì¬ì˜ í™˜ìœ¨ì€ ë‹¤ì–‘í•œ ê²½ì œì  ìš”ì¸ê³¼ ì •ì±…ì  ê²°ì •ì— ì˜í•´ ì˜í–¥ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. í–¥í›„ ê²½ì œ ìƒí™©ê³¼ ì •ì±… ë³€í™”ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ê°œì¸ê³¼ ê¸°ì—…ì€ í™˜ìœ¨ ë³€ë™ì— ë”°ë¥¸ ìœ„í—˜ ê´€ë¦¬ì™€ ê¸°íšŒ í¬ì°©ì„ ìœ„í•´ ì „ë¬¸ì ì¸ ë¶„ì„ê³¼ ì „ëµì  ê³„íšì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1ë‹¬ëŸ¬ = {data['rates']['KRW']}ì›\"\n",
    "\n",
    "# Partial Prompt í™œìš©\n",
    "prompt = PromptTemplate(\n",
    "    template=\"í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\",\n",
    "    input_variables=[],  # ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ìë™ ë°˜ì˜\n",
    ")\n",
    "\n",
    "# LLM ëª¨ë¸ ì„¤ì •\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\n",
    "response = llm.invoke(prompt.format())\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\" í”„ë¡¬í”„íŠ¸:\", prompt.format())\n",
    "print(\" ëª¨ë¸ ì‘ë‹µ:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

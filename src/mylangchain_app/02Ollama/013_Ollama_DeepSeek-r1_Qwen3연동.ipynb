{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry langchain langchain-ollama langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 Ollama로 설치한 deepseek-r1 모델과 ExaOne3 또는 Qwen3 모델을 사용하기\n",
    "##### ollama run deepseek-r1:7b\n",
    "##### ollama run exaone3.5\n",
    "##### ollama run qwen3:1.7b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최신버전 LangChain에서는 ChatOllama와 RunnableSequence(prompt | llm) 를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (영어로 질문, invoke()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "To make a fair comparison, it's helpful to have them written with the same number of decimal places. So, I can rewrite 9.9 as 9.90.\n",
      "\n",
      "Now that both numbers are in the hundredths place, I can compare each corresponding digit starting from the left.\n",
      "\n",
      "The units place for both numbers is 9, so they are equal there.\n",
      "\n",
      "Next, look at the tenths place: 9 (from 9.90) and 1 (from 9.11). Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Align the Decimal Places\n",
      "First, it might be easier to compare the numbers if they have the same number of decimal places. Let's rewrite **9.9** as **9.90**:\n",
      "\n",
      "\\[\n",
      "9.9 = 9.90\n",
      "\\]\n",
      "\n",
      "Now both numbers are:\n",
      "- **9.90**\n",
      "- **9.11**\n",
      "\n",
      "### Step 2: Compare Each Corresponding Digit\n",
      "Start by comparing the digits from left to right.\n",
      "\n",
      "1. **Units Place:**\n",
      "   - Both numbers have **9** in the units place.\n",
      "   \n",
      "2. **Tenths Place:**\n",
      "   - The tenths digit of **9.90** is **9**.\n",
      "   - The tenths digit of **9.11** is **1**.\n",
      "\n",
      "Since **9 > 1**, we can conclude that:\n",
      "\n",
      "\\[\n",
      "9.90 > 9.11\n",
      "\\]\n",
      "\n",
      "### Conclusion\n",
      "Therefore, **9.9** is larger than **9.11**.\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "try:\n",
    "    deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)    # 모델 호출\n",
    "    response = deepseek.invoke(\"which is bigger between 9.9 and 9.11?\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### qwen3:1.7b 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, invoke()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to compare these two numbers. Both are decimal numbers. Let me start by writing them down to visualize better.\n",
      "\n",
      "So, 9.9 is the same as 9.90, right? And 9.11 is 9.11. So, comparing them digit by digit. The whole numbers are both 9, so that's equal. Now, looking at the decimal parts. The first number, 9.9, has a decimal part of 0.9, which is 9 tenths. The second number, 9.11, has a decimal part of 0.11, which is 11 hundredths.\n",
      "\n",
      "Since 0.9 is equal to 0.90, and 0.11 is less than 0.90, then 9.9 is larger than 9.11. Wait, but let me make sure I'm not making a mistake here. Maybe I should convert them to fractions to check.\n",
      "\n",
      "9.9 is 99/10, and 9.11 is 911/100. Let's see, 99/10 is 9.9, and 911/100 is 9.11. To compare them, convert them to the same denominator. Let's find a common denominator. The denominators are 10 and 100. The least common denominator is 100. So, convert 99/10 to 990/100. Then, 911/100 is 911/100. So, 990/100 vs 911/100. Since 990 is greater than 911, 9.9 is greater than 9.11.\n",
      "\n",
      "Alternatively, I can think about the decimal places. 9.9 has one decimal place, and 9.11 has two. So, if I align them:\n",
      "\n",
      "9.90\n",
      "9.11\n",
      "\n",
      "Comparing the first decimal place: 9 vs 1. Since 9 is greater than 1, 9.90 is greater than 9.11. So that confirms it.\n",
      "\n",
      "Another way: subtract them. 9.9 minus 9.11 equals 0.79. So, 9.9 is 0.79 more than 9.11, which means 9.9 is bigger.\n",
      "\n",
      "I think that's solid. The answer should be 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9는 9.11보다 더 큰 수입니다.  \n",
      "**9.9**는 9.11보다 0.79 더 큰 값입니다.  \n",
      "따라서, **9.9가 더 큰 수입니다**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "try:\n",
    "    #exaone = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5, n_gpu_layers=0, batch_size=128)\n",
    "    exaone = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "    # 모델 호출\n",
    "    response = exaone.invoke(\"9.9와 9.11 중 무엇이 더 큰가요?\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (영어로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger between 9.9 and 9.11, I'll start by aligning their decimal places for a fair comparison.\n",
      "\n",
      "First, I can write 9.9 as 9.90 to match the two decimal places of 9.11.\n",
      "\n",
      "Next, I'll compare each corresponding digit from left to right.\n",
      "\n",
      "Both numbers have a 9 in the units place.\n",
      "\n",
      "Moving to the tenths place, both have a 9 and 1 respectively.\n",
      "\n",
      "Since 9 is greater than 1 in the tenths place, this indicates that 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Align the Decimal Places\n",
      "First, it's helpful to write both numbers with the same number of decimal places for a straightforward comparison.\n",
      "\n",
      "\\[\n",
      "9.9 \\quad \\text{can be written as} \\quad 9.90\n",
      "\\]\n",
      "\n",
      "Now we have:\n",
      "\n",
      "\\[\n",
      "9.90 \\quad \\text{and} \\quad 9.11\n",
      "\\]\n",
      "\n",
      "### Step 2: Compare Each Corresponding Digit\n",
      "Start from the leftmost digit and move to the right.\n",
      "\n",
      "- **Units Place:** Both numbers have a **9**.\n",
      "  \n",
      "- **Tenths Place:** \n",
      "  - **9.90** has a **9** in the tenths place.\n",
      "  - **9.11** has a **1** in the tenths place.\n",
      "  \n",
      "Since **9 > 1**, we can conclude that **9.90** is larger than **9.11**.\n",
      "\n",
      "### Conclusion\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger than } 9.11}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger between 9.9 and 9.11, I'll start by aligning their decimal places for a fair comparison.\n",
       "\n",
       "First, I can write 9.9 as 9.90 to match the two decimal places of 9.11.\n",
       "\n",
       "Next, I'll compare each corresponding digit from left to right.\n",
       "\n",
       "Both numbers have a 9 in the units place.\n",
       "\n",
       "Moving to the tenths place, both have a 9 and 1 respectively.\n",
       "\n",
       "Since 9 is greater than 1 in the tenths place, this indicates that 9.90 is larger than 9.11.\n",
       "\n",
       "Therefore, 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Align the Decimal Places\n",
       "First, it's helpful to write both numbers with the same number of decimal places for a straightforward comparison.\n",
       "\n",
       "\\[\n",
       "9.9 \\quad \\text{can be written as} \\quad 9.90\n",
       "\\]\n",
       "\n",
       "Now we have:\n",
       "\n",
       "\\[\n",
       "9.90 \\quad \\text{and} \\quad 9.11\n",
       "\\]\n",
       "\n",
       "### Step 2: Compare Each Corresponding Digit\n",
       "Start from the leftmost digit and move to the right.\n",
       "\n",
       "- **Units Place:** Both numbers have a **9**.\n",
       "  \n",
       "- **Tenths Place:** \n",
       "  - **9.90** has a **9** in the tenths place.\n",
       "  - **9.11** has a **1** in the tenths place.\n",
       "  \n",
       "Since **9 > 1**, we can conclude that **9.90** is larger than **9.11**.\n",
       "\n",
       "### Conclusion\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is bigger than } 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two decimal numbers: 9.9 and 9.11.\n",
      "\n",
      "To make a fair comparison, it's helpful to have them expressed with the same number of decimal places. So, I'll rewrite 9.9 as 9.90.\n",
      "\n",
      "Now, both numbers are written to two decimal places:\n",
      "- 9.90\n",
      "- 9.11\n",
      "\n",
      "Next, I'll compare each corresponding digit from left to right:\n",
      "- The integer parts (the digits before the decimal point) are both 9.\n",
      "- The tenths place: 9 vs. 1.\n",
      "  \n",
      "Since 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "</think>\n",
      "\n",
      " comparison of 9.9 and 9.11:\n",
      "\n",
      "To determine which number is larger between \\( 9.9 \\) and \\( 9.11 \\), we can compare them digit by digit after ensuring they have the same number of decimal places.\n",
      "\n",
      "1. **Express both numbers with two decimal places:**\n",
      "   - \\( 9.9 = 9.90 \\)\n",
      "   - \\( 9.11 \\)\n",
      "\n",
      "2. **Compare each corresponding digit from left to right:**\n",
      "   - **Integer part:** Both have the same integer value, which is **9**.\n",
      "   - **Tenths place:** Compare \\( 9 \\) (from \\( 9.90 \\)) and \\( 1 \\) (from \\( 9.11 \\)).\n",
      "     - Since \\( 9 > 1 \\), \\( 9.90 \\) is greater than \\( 9.11 \\).\n",
      "\n",
      "3. **Conclusion:**\n",
      "   - Therefore, \\( 9.9 \\) is larger than \\( 9.11 \\).\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9}\n",
      "\\]"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I need to compare the two decimal numbers: 9.9 and 9.11.\n",
       "\n",
       "To make a fair comparison, it's helpful to have them expressed with the same number of decimal places. So, I'll rewrite 9.9 as 9.90.\n",
       "\n",
       "Now, both numbers are written to two decimal places:\n",
       "- 9.90\n",
       "- 9.11\n",
       "\n",
       "Next, I'll compare each corresponding digit from left to right:\n",
       "- The integer parts (the digits before the decimal point) are both 9.\n",
       "- The tenths place: 9 vs. 1.\n",
       "  \n",
       "Since 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\n",
       "\n",
       "Therefore, 9.9 is greater than 9.11.\n",
       "</think>\n",
       "\n",
       " comparison of 9.9 and 9.11:\n",
       "\n",
       "To determine which number is larger between \\( 9.9 \\) and \\( 9.11 \\), we can compare them digit by digit after ensuring they have the same number of decimal places.\n",
       "\n",
       "1. **Express both numbers with two decimal places:**\n",
       "   - \\( 9.9 = 9.90 \\)\n",
       "   - \\( 9.11 \\)\n",
       "\n",
       "2. **Compare each corresponding digit from left to right:**\n",
       "   - **Integer part:** Both have the same integer value, which is **9**.\n",
       "   - **Tenths place:** Compare \\( 9 \\) (from \\( 9.90 \\)) and \\( 1 \\) (from \\( 9.11 \\)).\n",
       "     - Since \\( 9 > 1 \\), \\( 9.90 \\) is greater than \\( 9.11 \\).\n",
       "\n",
       "3. **Conclusion:**\n",
       "   - Therefore, \\( 9.9 \\) is larger than \\( 9.11 \\).\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### qwen3:1.7b 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The question is asking which is bigger between 9.9 and 9.11. Hmm, both numbers are decimals. Let me think about how to compare them.\n",
      "\n",
      "First, I know that when comparing decimals, you start by looking at the digits from left to right. So, let's write them out:\n",
      "\n",
      "9.9 is the same as 9.90, right? Because adding a zero at the end doesn't change the value. Similarly, 9.11 is 9.110. So, comparing 9.90 and 9.110.\n",
      "\n",
      "Now, the first digit after the decimal is 9 in the first number and 1 in the second. Wait, no, hold on. Wait, 9.9 is 9.90, so the first digit after the decimal is 9, and the second number, 9.11, has 1 as the first decimal digit. So, 9.90 is 9.90, which is more than 9.11 because the first digit after the decimal in 9.90 is 9, which is greater than 1. So, even if the second number has more digits, the first digit is already higher.\n",
      "\n",
      "Wait, but maybe I should check the exact digits. Let's break it down:\n",
      "\n",
      "9.9 is 9 + 0.9, and 9.11 is 9 + 0.11. So, comparing the whole numbers, both are 9. Then, comparing the decimal parts: 0.9 vs 0.11. Since 0.9 is greater than 0.11, 9.9 is greater than 9.11.\n",
      "\n",
      "Alternatively, if I convert them to fractions, 9.9 is 99/10 and 9.11 is 911/100. Let's see: 99/10 is 9.9 and 911/100 is 9.11. To compare these fractions, multiply numerator and denominator to make them the same. Let's find a common denominator, which would be 100. So, 99/10 = 990/100, and 911/100 is 911/100. Comparing 990 and 911, 990 is larger. Therefore, 9.9 is larger than 9.11.\n",
      "\n",
      "Another way: think about the decimal places. 9.9 has two decimal places, and 9.11 has three. If you align them, 9.90 vs 9.11. The first digit after the decimal is 9 vs 1. Since 9 is greater than 1, 9.90 is greater than 9.11.\n",
      "\n",
      "So, regardless of the method, 9.9 is bigger than 9.11. I don't think there's any trick here. The question is straightforward. The answer should be 9.9.\n",
      "</think>\n",
      "\n",
      "9.9는 9.11보다 더 크다. 두 수를 비교할 때, 정수 부분은 같고 소수 부분을 비교하면 9.9는 0.9, 9.11은 0.11을 가진다. 0.9는 0.11보다 큰 것이므로, 9.9는 9.11보다 더 큰 수이다.\n",
      "\n",
      "**답변:** 9.9가 더 크다."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's see. The question is asking which is bigger between 9.9 and 9.11. Hmm, both numbers are decimals. Let me think about how to compare them.\n",
       "\n",
       "First, I know that when comparing decimals, you start by looking at the digits from left to right. So, let's write them out:\n",
       "\n",
       "9.9 is the same as 9.90, right? Because adding a zero at the end doesn't change the value. Similarly, 9.11 is 9.110. So, comparing 9.90 and 9.110.\n",
       "\n",
       "Now, the first digit after the decimal is 9 in the first number and 1 in the second. Wait, no, hold on. Wait, 9.9 is 9.90, so the first digit after the decimal is 9, and the second number, 9.11, has 1 as the first decimal digit. So, 9.90 is 9.90, which is more than 9.11 because the first digit after the decimal in 9.90 is 9, which is greater than 1. So, even if the second number has more digits, the first digit is already higher.\n",
       "\n",
       "Wait, but maybe I should check the exact digits. Let's break it down:\n",
       "\n",
       "9.9 is 9 + 0.9, and 9.11 is 9 + 0.11. So, comparing the whole numbers, both are 9. Then, comparing the decimal parts: 0.9 vs 0.11. Since 0.9 is greater than 0.11, 9.9 is greater than 9.11.\n",
       "\n",
       "Alternatively, if I convert them to fractions, 9.9 is 99/10 and 9.11 is 911/100. Let's see: 99/10 is 9.9 and 911/100 is 9.11. To compare these fractions, multiply numerator and denominator to make them the same. Let's find a common denominator, which would be 100. So, 99/10 = 990/100, and 911/100 is 911/100. Comparing 990 and 911, 990 is larger. Therefore, 9.9 is larger than 9.11.\n",
       "\n",
       "Another way: think about the decimal places. 9.9 has two decimal places, and 9.11 has three. If you align them, 9.90 vs 9.11. The first digit after the decimal is 9 vs 1. Since 9 is greater than 1, 9.90 is greater than 9.11.\n",
       "\n",
       "So, regardless of the method, 9.9 is bigger than 9.11. I don't think there's any trick here. The question is straightforward. The answer should be 9.9.\n",
       "</think>\n",
       "\n",
       "9.9는 9.11보다 더 크다. 두 수를 비교할 때, 정수 부분은 같고 소수 부분을 비교하면 9.9는 0.9, 9.11은 0.11을 가진다. 0.9는 0.11보다 큰 것이므로, 9.9는 9.11보다 더 큰 수이다.\n",
       "\n",
       "**답변:** 9.9가 더 크다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepSeek의 추론 능력과 Qwen3의 한글 생성 능력 결합하기\n",
    "* DeepSeek는 태그 안에서 이루어지는 추론을 기반으로 다른 LLM 대비 높은 성능을 발휘합니다.\n",
    "* 하지만 Ollama에서 제공하는 deepseek r1 모델은 한국어 생성 능력이 부족합니다.\n",
    "* DeepSeek의 추론 능력과 Qwen3의 한글 생성 능력 결합해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='qwen3:1.7b' temperature=0.7\n"
     ]
    }
   ],
   "source": [
    "#generation_model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.7)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.7)\n",
    "print(generation_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph 로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문에 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문에 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.graph.state.StateGraph'>\n",
      "<class 'langgraph.graph.state.CompiledStateGraph'>\n"
     ]
    }
   ],
   "source": [
    "#DeepSeek를 통해서 추론 부분까지만 생성합니다. Node\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다. Node\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "print(type(graph_builder))\n",
    "print(type(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers start with 9, so the whole numbers are equal. The next part is the decimal part. \n",
      "\n",
      "For 9.9, the decimal is 0.9, and for 9.11, it's 0.11. Wait, but when comparing decimals, you go digit by digit. So, after the decimal, the first digit in 9.9 is 9, and in 9.11 it's 1. Since 9 is greater than 1, that means 9.9 is larger than 9.11. \n",
      "\n",
      "But wait, maybe I should check if there's any trick here. Sometimes people might confuse the decimal places, but in this case, both numbers have two decimal places. So 9.9 is 9.90, and 9.11 is 9.11. Comparing 9.90 vs 9.11, the tenths place of 9.90 is 9, which is more than 1 in 9.11. So yes, 9.9 is bigger. \n",
      "\n",
      "I should make sure to explain the steps clearly without using technical terms. Maybe start by noting the whole numbers are the same, then compare the decimals. Highlight the key point that the tenths digit in 9.9 is higher than in 9.11. That should make it straightforward for the user.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 숫자는 **9.9**입니다.  \n",
      "두 숫자는 모두 9를 기준으로 시작하는 것으로, 소수점 부분을 비교해야 합니다.  \n",
      "9.9는 9.90으로 바꾸면, 9.11은 9.11로 비교할 수 있습니다.  \n",
      "소수점 첫째 자리에서는 9와 1을 비교합니다. 9는 1보다 클 것이므로, **9.9는 9.11보다 더 큰 숫자입니다**.  \n",
      "\n",
      "이러한 비교는 숫자의 자릿수와 값의 규칙에 기반합니다. 소수점 이전의 숫자가 같고, 소수점 이후의 숫자를 차례로 비교하는 것이 중요합니다.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align them by adding an extra decimal place to 9.9, making it 9.90.\\n\\nNow, both numbers are 9.90 and 9.11.\\n\\nComparing each digit from left to right:\\n\\n- The whole number part is the same (9).\\n- In the tenths place, 9 is greater than 1.\\n  \\nSince 9 in the tenths place of 9.90 is larger than 1 in the tenths place of 9.11, it follows that 9.90 is greater than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"<think>\\nOkay, the user is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers start with 9, so the whole numbers are equal. The next part is the decimal part. \\n\\nFor 9.9, the decimal is 0.9, and for 9.11, it's 0.11. Wait, but when comparing decimals, you go digit by digit. So, after the decimal, the first digit in 9.9 is 9, and in 9.11 it's 1. Since 9 is greater than 1, that means 9.9 is larger than 9.11. \\n\\nBut wait, maybe I should check if there's any trick here. Sometimes people might confuse the decimal places, but in this case, both numbers have two decimal places. So 9.9 is 9.90, and 9.11 is 9.11. Comparing 9.90 vs 9.11, the tenths place of 9.90 is 9, which is more than 1 in 9.11. So yes, 9.9 is bigger. \\n\\nI should make sure to explain the steps clearly without using technical terms. Maybe start by noting the whole numbers are the same, then compare the decimals. Highlight the key point that the tenths digit in 9.9 is higher than in 9.11. That should make it straightforward for the user.\\n</think>\\n\\n9.9와 9.11 중 더 큰 숫자는 **9.9**입니다.  \\n두 숫자는 모두 9를 기준으로 시작하는 것으로, 소수점 부분을 비교해야 합니다.  \\n9.9는 9.90으로 바꾸면, 9.11은 9.11로 비교할 수 있습니다.  \\n소수점 첫째 자리에서는 9와 1을 비교합니다. 9는 1보다 클 것이므로, **9.9는 9.11보다 더 큰 숫자입니다**.  \\n\\n이러한 비교는 숫자의 자릿수와 값의 규칙에 기반합니다. 소수점 이전의 숫자가 같고, 소수점 이후의 숫자를 차례로 비교하는 것이 중요합니다.\"}\n",
      "==> 생성된 답변: \n",
      "\n",
      "<think>\n",
      "Okay, the user is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers start with 9, so the whole numbers are equal. The next part is the decimal part. \n",
      "\n",
      "For 9.9, the decimal is 0.9, and for 9.11, it's 0.11. Wait, but when comparing decimals, you go digit by digit. So, after the decimal, the first digit in 9.9 is 9, and in 9.11 it's 1. Since 9 is greater than 1, that means 9.9 is larger than 9.11. \n",
      "\n",
      "But wait, maybe I should check if there's any trick here. Sometimes people might confuse the decimal places, but in this case, both numbers have two decimal places. So 9.9 is 9.90, and 9.11 is 9.11. Comparing 9.90 vs 9.11, the tenths place of 9.90 is 9, which is more than 1 in 9.11. So yes, 9.9 is bigger. \n",
      "\n",
      "I should make sure to explain the steps clearly without using technical terms. Maybe start by noting the whole numbers are the same, then compare the decimals. Highlight the key point that the tenths digit in 9.9 is higher than in 9.11. That should make it straightforward for the user.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 숫자는 **9.9**입니다.  \n",
      "두 숫자는 모두 9를 기준으로 시작하는 것으로, 소수점 부분을 비교해야 합니다.  \n",
      "9.9는 9.90으로 바꾸면, 9.11은 9.11로 비교할 수 있습니다.  \n",
      "소수점 첫째 자리에서는 9와 1을 비교합니다. 9는 1보다 클 것이므로, **9.9는 9.11보다 더 큰 숫자입니다**.  \n",
      "\n",
      "이러한 비교는 숫자의 자릿수와 값의 규칙에 기반합니다. 소수점 이전의 숫자가 같고, 소수점 이후의 숫자를 차례로 비교하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        #graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LangGraph 실행 시작\n",
      "==================================================\n",
      "[DEBUG] 입력 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 529\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align them by adding an extra decimal place ...\n",
      "[DEBUG] generate 함수 - 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] generate 함수 - 추론 길이: 529\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align them by adding an extra decimal place ...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "오류 발생: model \"qwen2.5:1.5b\" not found, try pulling it first (status code: 404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12372\\2706828140.py\", line 133, in main\n",
      "    result = graph.invoke(inputs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\pregel\\main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "                 ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\pregel\\main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\pregel\\_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\pregel\\_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12372\\2706828140.py\", line 109, in generate\n",
      "    response = generation_model.invoke(messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 824, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 759, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 846, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_ollama\\chat_models.py\", line 745, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\ollama\\_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"qwen2.5:1.5b\" not found, try pulling it first (status code: 404)\n",
      "During task with name 'generate' and id 'a0571427-89c0-6f11-6726-fda7ed3842ee'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 초기화 - 한글 처리 개선\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# LangGraph State 정의\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        return text.decode('utf-8', errors='ignore')\n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 구성 및 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "def main():\n",
    "    # 입력 데이터\n",
    "    inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"LangGraph 실행 시작\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # invoke()를 사용하여 그래프 호출\n",
    "        result = graph.invoke(inputs)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"실행 결과\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"전체 결과: {result}\")\n",
    "        print(f\"최종 답변: {result.get('answer', '답변 없음')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 입력 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align them by adding an extra decimal place to 9.9, making it 9.90.\n",
      "\n",
      "Now, both numbers are 9.90 and 9.11.\n",
      "\n",
      "Comparing each digit from left to right:\n",
      "\n",
      "- The whole number part is the same (9).\n",
      "- In the tenths place, 9 is greater than 1.\n",
      "  \n",
      "Since 9 in the tenths place of 9.90 is larger than 1 in the tenths place of 9.11, it follows that 9.90 is greater than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 556\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align them by adding an extra decimal place ...\n",
      "[DEBUG] generate 함수 - 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] generate 함수 - 추론 길이: 556\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align them by adding an extra decimal place ...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <coroutine object _render_mermaid_using_pyppeteer at 0x00000225D79BCFB0>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n",
      "Exception ignored in: <coroutine object _render_mermaid_using_pyppeteer at 0x00000225D79BCFB0>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n",
      "Exception ignored in: <coroutine object _render_mermaid_using_pyppeteer at 0x00000225D79155A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n",
      "Exception ignored in: <coroutine object _render_mermaid_using_pyppeteer at 0x00000225D7915210>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model \"qwen2.5:1.5b\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m9.9와 9.11 중 무엇이 더 큰가요?\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph.astream_events(inputs, version=\u001b[33m\"\u001b[39m\u001b[33mv2\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      4\u001b[39m     kind = event[\u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kind == \u001b[33m\"\u001b[39m\u001b[33mon_chat_model_stream\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1540\u001b[39m, in \u001b[36mRunnable.astream_events\u001b[39m\u001b[34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m   1539\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[32m   1541\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py:1034\u001b[39m, in \u001b[36m_astream_events_implementation_v2\u001b[39m\u001b[34m(runnable, value, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# Await it anyway, to run any cleanup code, and propagate any exceptions\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.suppress(asyncio.CancelledError):\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m task\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py:989\u001b[39m, in \u001b[36m_astream_events_implementation_v2.<locals>.consume_astream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    987\u001b[39m     \u001b[38;5;66;03m# if astream also calls tap_output_aiter this will be a no-op\u001b[39;00m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(runnable.astream(value, config, **kwargs)) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m event_streamer.tap_output_aiter(run_id, stream):\n\u001b[32m    990\u001b[39m             \u001b[38;5;66;03m# All the content will be picked up\u001b[39;00m\n\u001b[32m    991\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py:213\u001b[39m, in \u001b[36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mT\u001b[39m\u001b[33m\"\u001b[39m, first)\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# consume the rest of the output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m output:\n\u001b[32m    214\u001b[39m     \u001b[38;5;28mself\u001b[39m._send(\n\u001b[32m    215\u001b[39m         {**event, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mchunk\u001b[39m\u001b[33m\"\u001b[39m: chunk}},\n\u001b[32m    216\u001b[39m         run_info[\u001b[33m\"\u001b[39m\u001b[33mrun_type\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    217\u001b[39m     )\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\pregel\\main.py:2939\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2938\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2939\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2940\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2941\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2942\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2943\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2944\u001b[39m ):\n\u001b[32m   2945\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2946\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2947\u001b[39m         stream_mode,\n\u001b[32m   2948\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2951\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2952\u001b[39m     ):\n\u001b[32m   2953\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:132\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m task.proc.astream(task.input, config):\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:840\u001b[39m, in \u001b[36mRunnableSeq.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m             aiterator = h.tap_output_aiter(\n\u001b[32m    837\u001b[39m                 run_manager.run_id, aiterator\n\u001b[32m    838\u001b[39m             )\n\u001b[32m    839\u001b[39m \u001b[38;5;66;03m# consume into final output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    841\u001b[39m     _consume_aiter(aiterator), context=context\n\u001b[32m    842\u001b[39m )\n\u001b[32m    843\u001b[39m \u001b[38;5;66;03m# sequence doesn't emit output, yield to mark as generator\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:905\u001b[39m, in \u001b[36m_consume_aiter\u001b[39m\u001b[34m(it)\u001b[39m\n\u001b[32m    903\u001b[39m output: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    904\u001b[39m add_supported = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[32m    906\u001b[39m     \u001b[38;5;66;03m# collect final output\u001b[39;00m\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m add_supported:\n\u001b[32m    908\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py:190\u001b[39m, in \u001b[36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    188\u001b[39m tap = \u001b[38;5;28mself\u001b[39m.is_tapped.setdefault(run_id, sentinel)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m first = \u001b[38;5;28;01mawait\u001b[39;00m py_anext(output, default=sentinel)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\utils\\aiter.py:78\u001b[39m, in \u001b[36mpy_anext.<locals>.anext_impl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manext_impl\u001b[39m() -> Union[T, Any]:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# The C code is way more low-level than this, as it implements\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# all methods of the iterator protocol. In this implementation\u001b[39;00m\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# we're relying on higher-level coroutine concepts, but that's\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# exactly what we want -- crosstest pure-Python high-level\u001b[39;00m\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# implementation and low-level C anext() iterators.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[34m__anext__\u001b[39m(iterator)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1613\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1610\u001b[39m final: Input\n\u001b[32m   1611\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1613\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[32m   1614\u001b[39m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[32m   1615\u001b[39m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[32m   1616\u001b[39m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[32m   1618\u001b[39m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[32m   1619\u001b[39m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[32m   1621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[32m   1622\u001b[39m         final = ichunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1176\u001b[39m, in \u001b[36mRunnable.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1157\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mastream\u001b[39m(\n\u001b[32m   1158\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m   1160\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1161\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   1162\u001b[39m ) -> AsyncIterator[Output]:\n\u001b[32m   1163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Default implementation of ``astream``, which calls ``ainvoke``.\u001b[39;00m\n\u001b[32m   1164\u001b[39m \n\u001b[32m   1165\u001b[39m \u001b[33;03m    Subclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1174\u001b[39m \n\u001b[32m   1175\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1176\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:474\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\config.py:611\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(executor_or_config, func, *args, **kwargs)\u001b[39m\n\u001b[32m    607\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    610\u001b[39m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(\n\u001b[32m    612\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    613\u001b[39m         cast(\u001b[33m\"\u001b[39m\u001b[33mCallable[..., T]\u001b[39m\u001b[33m\"\u001b[39m, partial(copy_context().run, wrapper)),\n\u001b[32m    614\u001b[39m     )\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\runnables\\config.py:602\u001b[39m, in \u001b[36mrun_in_executor.<locals>.wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m() -> T:\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    604\u001b[39m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[32m    605\u001b[39m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[32m    606\u001b[39m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[32m    607\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    102\u001b[39m messages = answer_prompt.invoke({\n\u001b[32m    103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question, \n\u001b[32m    104\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mthinking\u001b[39m\u001b[33m\"\u001b[39m: thinking\n\u001b[32m    105\u001b[39m })\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] 프롬프트 메시지 생성 완료\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m response = \u001b[43mgeneration_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m answer_content = ensure_utf8_string(response.content)\n\u001b[32m    112\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] 최종 응답 타입: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(response.content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1023\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1020\u001b[39m     **kwargs: Any,\n\u001b[32m   1021\u001b[39m ) -> LLMResult:\n\u001b[32m   1022\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:840\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    838\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    839\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m         )\n\u001b[32m    847\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    848\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1078\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream(\n\u001b[32m   1073\u001b[39m     async_api=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1074\u001b[39m     run_manager=run_manager,\n\u001b[32m   1075\u001b[39m     **kwargs,\n\u001b[32m   1076\u001b[39m ):\n\u001b[32m   1077\u001b[39m     chunks: \u001b[38;5;28mlist\u001b[39m[ChatGenerationChunk] = []\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_gen_info_and_msg_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_ollama\\chat_models.py:907\u001b[39m, in \u001b[36mChatOllama._stream\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stream\u001b[39m(\n\u001b[32m    901\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    902\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    905\u001b[39m     **kwargs: Any,\n\u001b[32m    906\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_llm_new_token\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m                \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_ollama\\chat_models.py:846\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m    840\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    841\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    842\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    843\u001b[39m     **kwargs: Any,\n\u001b[32m    844\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m    845\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m--> \u001b[39m\u001b[32m846\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    852\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\langchain_ollama\\chat_models.py:745\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    744\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\ollama\\_client.py:170\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m   e.response.read()\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.iter_lines():\n\u001b[32m    173\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: model \"qwen2.5:1.5b\" not found, try pulling it first (status code: 404)",
      "During task with name 'generate' and id '8e01391a-5343-6e87-4535-4997674f5e1c'"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:348: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n",
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\gradio\\utils.py:1052: UserWarning: Expected 2 arguments for function <function chatbot_interface at 0x00000225D29A9800>, received 1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\gradio\\utils.py:1056: UserWarning: Expected at least 2 arguments for function <function chatbot_interface at 0x00000225D29A9800>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.12\\Lib\\site-packages\\gradio\\helpers.py:1060: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 입력 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 556\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align them by adding an extra decimal place ...\n",
      "[DEBUG] generate 함수 - 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] generate 함수 - 추론 길이: 556\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align them by adding an extra decimal place ...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 897\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. The assistant's reasoning process was to compare the numbers step by step. First, they noticed both have the same whole number part, 9. Then, they aligned them by adding a zero to 9.9 to make it 9.90. Comparing the tenths place, 9 is greater than 1, so 9.90 is bigger. Therefore, 9.9 is greater than 9.11.\n",
      "\n",
      "I need to present this in Korean without mentioning the reasoning process. So, start by stating that 9.9 is larger than 9.11 because the tenths place of 9.9 (which is 9) is greater than 1 in 9.11. Make sure to explain it clearly and naturally, without listing the steps. Maybe use a conversational tone. Check for any errors and ensure the answer is concise.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 9.9는 더 큰 수입니다. 두 수의 정수 부분은 모두 9이고, 소수 부분에서는 9.9는 9를, 9.11은 1을 가지고 있습니다. 따라서 9.9의 소수 부분(9)이 9.11의 소수 부분(1)보다 크므로, 9.9가 더 큰 수입니다.\n",
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen3:1.7b\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문에 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # 문자열이지만 인코딩 문제가 있을 수 있는 경우 처리\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # 문자열을 UTF-8로 인코딩했다가 다시 디코딩하여 정리\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 생성 함수: 상태(State) 간의 흐름을 정의\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio 인터페이스 생성 및 실행\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "def launch_gradio():\n",
    "    iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "    iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #iface.launch()\n",
    "    launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
